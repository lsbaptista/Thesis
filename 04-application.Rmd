# Aplicação a dados do modelo
\newpage

## Introdução
```{r,include=FALSE}
Sys.setlocale("LC_ALL","English")
rm(list = ls())
knitr::opts_chunk$set(tidy=FALSE, message = FALSE,warning =  FALSE,fig.align = "center",fig.show = "hold")
library(readxl)
library(portes)
library(fGarch)
library(MFTSR)
library(ggfortify)
library(tidyquant)
library(kableExtra)
library(tidyverse)
library(RQuantLib)
library(timeSeries)
library(fPortfolio)
library(rugarch)
```
A análise contempla, numa primeira fase, extrair 4 empresas constituintes do Euro Stoxx 50^[https://etf.invesco.com/pt/institutional/en/product/invesco-euro-stoxx-50-ucits-etf-acc/index-components], sendo que essa escolha foi realizado tendo em consideração a diversificação do portfolio por sector de actividade e por presença geográfica, possibilitando deste modo minimizar a correlação que possa existir, apostando em sectores considerados cíclicos e também em sectores defensivos, estando as empresas seleccionadas representados na figura \@ref(fig:empresas), onde podemos verificar o sector de actividade e a sua cotação no último dia considerado.

```{r, empresas,echo=FALSE,fig.cap='Empresas extraidas do Euro Stoxx 50',out.width="100%"}
knitr::include_graphics("image/cotacao.png")
```
\FloatBarrier
\centering 
Fonte: Elaboração própria.

\justifying
\bigskip
O intervalo temporal utilizado na obtenção dos dados foi de 01/06/2016 a 01/06/2020, contemplando 4
anos de dados históricos, tendo sido obtidos a partir do Yahoo.finance, perfazendo 1014 dias de negociação. De acordo com @NG2006 "...recomenda-se usar 1000 amostras para a estimação do modelo GARCH convencional". Ainda considerando @smallsample "...foi demonstrado que um nível elevado de persistência obtido em modelos GARCH(1,1) usando um número elevado de observações têm auto-correlação menor do que as estimações sugeridas com amostras pequenas"

Resumindo as várias etapas e de acordo com a figura \@ref(fig:processo), após a obtenção e tratamento dos dados é realizada uma análise descritiva dos mesmos, sendo depois estimada a volatilidade para o último dia de acordo com os modelos em estudo. O modelo de simulação integra esses valores obtidos de variância para cada um dos activos e de acordo com o modelo utilizado, sendo constituído um portfolio que minimize a variância com base em 80 dias de simulação de preços, sendo as restrições aplicáveis ao modelo a não possibilidade de posições curtas, investir em activos sem risco e empréstimo de capital.
 
```{r, processo,echo=FALSE,fig.cap='Modelação do processo',out.width="60%"}
knitr::include_graphics("image/modelação.png")
```
\FloatBarrier
\centering 
Fonte: Elaboração própria.

\justifying
Ao longo do trabalho e sempre que se verifique necessário será disponibilizado no próprio conteúdo do trabalho o código R utilizado para tratamento e obtenção dos dados apresentados. As tabelas originadas que contemplam todo o *output* de um determinado código serão colocadas em anexo.

## Análise descritiva dos dados

Na tabela \@ref(tab:sogtail) estão representados os valores para as acções da empresa Société Générale nos últimos 5 dias em análise, sendo os valores representativos do preço de fecho e do preço ajustado. O preço ajustado contempla a remuneração do acionista com dividendos a serem pagos ou pagos e também o ajuste a ocorrência de *split* das acções. No calculo do retorno utiliza-se o preço ajustado.

```{r,echo=FALSE}
TEF.MC<-read_xlsx ("data/TEF.MC.xlsx")
TEF.MC <- xts(TEF.MC[, -1], order.by=as.Date(TEF.MC$Date))
GLE.PA<-read_xlsx ("data/GLE.PA.xlsx")
GLE.PA <- xts(GLE.PA[, -1], order.by=as.Date(GLE.PA$Date))
ENEL.MI<-read_xlsx ("data/ENEL.MI.xlsx")
ENEL.MI <- xts(ENEL.MI[, -1], order.by=as.Date(ENEL.MI$Date))
AIR.PA<-read_xlsx ("data/AIR.PA.xlsx")
AIR.PA <- xts(AIR.PA[, -1], order.by=as.Date(AIR.PA$Date))
```

```{r,sogtail,echo=FALSE}
gle<-tail(GLE.PA)
gle <- knitr::kable(
  gle, caption = "Société Générale.Euronext Paris-25-05-2020/01-06-2020",
  booktabs = TRUE)
kable_styling(gle, latex_options = "hold_position", position = "center")
```
\FloatBarrier
\centering 
Fonte:https://finance.yahoo.com/

\justifying
\bigskip
Na figura \@ref(fig:cotacao) temos a evolução da cotação das acções das empresas nos últimos 4 anos, verificando-se no inicio do ano de 2020 uma queda abrupta nas cotações, devido as possíveis repercussões económicas negativas com o alastrar de infecções de COVID-19 a nível mundial, tendo originado nalgumas destas empresas quedas superiores a 50% do seu valor.

```{r,echo=FALSE, cotacao, fig.cap='Evolução das cotações nos últimos 5 anos',fig.topcaption = TRUE,out.width = "45%"}
par(mfrow  = c(2,2))
chartSeries(TEF.MC,theme = chartTheme("white",up.col="darkgreen"))
chartSeries(ENEL.MI,theme = chartTheme("white",up.col="dodgerblue3"))
chartSeries(GLE.PA,theme = chartTheme("white",up.col="darkorange"))
chartSeries(AIR.PA,theme = chartTheme("white",up.col="deeppink"))
```
\FloatBarrier
\centering 
Fonte: Elaboração própria.

\justifying
\bigskip

```{r,fecho, echo=FALSE}
newdata <- merge(TEF.MC$`Adj Close`,ENEL.MI$`Adj Close`,GLE.PA$`Adj Close`,AIR.PA$`Adj Close`)
newdata <- na.omit(newdata)
colnames(newdata) <- c("TEF.MC","ENEL.MI","GLE.PA","AIR.PA")
```

Com a função *summary* agrega-se na tabela \@ref(tab:estcot) algumas estatísticas referentes a variação das cotações, apresentando os valores mínimos e máximos, assim como quartis e a média. Os valores referentes a média e mediana ilustram que existem empresas cuja distribuição dos valores apresentam uma cauda mais à direita(ENI), enquanto outras apresentam uma cauda mais a esquerda (GLE), de acordo com os dados dos últimos 4 anos.

\scriptsize
```{r,estcot, wrapper=TRUE}
#R CODE
pt <-summary(as.data.frame(newdata))
p <- knitr::kable(
  pt, caption = "Estatísticas das cotações",
  booktabs = TRUE)
kable_styling(p, latex_options = "hold_position", position = "center")
```
\normalsize
\FloatBarrier
\centering 
Fonte:Elaboração própria.

\justifying
\bigskip

O cálculo dos retornos diários é efectuado com a função *dailyreturn*, sendo aplicada a função dos retornos logarítmicos de acordo com a equação \@ref(eq:logRet). No gráfico \@ref(fig:volatilidade) estão representados os gráficos referentes aos retornos diários logarítmicos para cada uma das empresa.

\scriptsize
```{r}
#R CODE
returns <- as.xts(data.frame(round(dailyReturn(newdata$TEF.MC,type="log"),4),
                             round(dailyReturn(newdata$ENEL.MI,type="log"),4),
                             round(dailyReturn(newdata$GLE.PA,type="log"),4),
                             round(dailyReturn(newdata$AIR.PA,type="log"),4)))
returns <- returns[-1,]
colnames(returns)<-c("TEF.RET","ENEL.RET","GLE.RET","AIR.RET")
```
\normalsize

```{r,volatilidade,echo=FALSE, fig.cap='Volatilidade nos últimos 4 anos',fig.topcaption = TRUE,out.width = "45%"}
par(mfrow  = c(2,2))
autoplot(returns$TEF.RET,ts.colour = 'darkgreen') + ggtitle("TEF") + 
  ylab("Log return") + xlab("anos") + theme(plot.title = element_text(hjust = 0.5))
autoplot(returns$ENEL.RET,ts.colour = 'dodgerblue3') + ggtitle("ENEL") + 
  ylab("Log return") + xlab("anos") + theme(plot.title = element_text(hjust = 0.5))
autoplot(returns$GLE.RET,ts.colour = 'darkorange') + ggtitle("GLE") + 
  ylab("Log return") + xlab("anos") + theme(plot.title = element_text(hjust = 0.5))
autoplot(returns$AIR.RET,ts.colour = 'deeppink') + ggtitle("AIR") + 
  ylab("Log return") + xlab("anos") + theme(plot.title = element_text(hjust = 0.5))
```
\FloatBarrier
\centering 
Fonte:Elaboração própria.

\justifying
\bigskip

Na matrix de correlações \@ref(tab:correl) dos retornos podemos verificar que a correlação entre os vários activos financeiros pode ser considerada como apresentando uma correlação fraca ou moderada^[De acordo com os valores apresentados em https://pt.wikipedia.org/wiki/Coeficiente_de_correla%C3%A7%C3%A3o_de_Pearson a 31/01/2021].

\scriptsize
```{r}
#R CODE
retcor <- cor(returns,method = "pearson")
```
\normalsize

```{r,correl, echo=FALSE}
retcork <- knitr::kable(
  retcor, caption = "Matrix correlação dos retornos",
  booktabs = TRUE)
kable_styling(retcork , latex_options = "hold_position", position = "center")
```
\FloatBarrier
\centering 
Fonte:Elaboração própria.

\justifying
\bigskip

A analise descritiva dos retornos é apresentada na tabela \@ref(tab:statret), sendo que todos apresentam uma assimetria negativa ou à esquerda, em que a cauda da distribuição nestes casos aponta para a esquerda. A curtose apresenta valores elevados e superiores a 3, sendo que as distribuições tem cauda mais pesadas do que a da distribuição normal. Neste caso as distribuições são ditas *leptokurtic* sendo uma característica comum na distribuição dos retornos das acções. 

As volatilidades apresentadas, assim como o retorno, são valores anuais e com peso igual atribuído a cada uma das observações.

\scriptsize
```{r}
#R CODE
retannual <- function(x) {  
  Return.annualized(x,geometric = FALSE)}
retvol <- function(x) {  
  StdDev.annualized(x,geometric = FALSE)}

ETFStats <- do.call(data.frame, 
                    list("média anual"= round(apply(returns, 2,retannual),6),
                         "volatilidade anual"= round(apply(returns, 2, retvol),6),
                         mediana = round(apply(returns, 2, median),6),
                         skewness = round(apply(returns, 2, skewness),6),
                         kurtosis = round(apply(returns, 2, kurtosis),6),
                         min = round(apply(returns, 2, min),6),
                         max = round(apply(returns, 2, max),6)))
```
\normalsize

```{r,statret, echo=FALSE}
ETFStats<-t(ETFStats)
ETFStatsk<- knitr::kable(
  ETFStats, caption = "Estatísticas dos retornos",
  booktabs = TRUE)
kable_styling(ETFStatsk , latex_options = "hold_position", position = "center")
```
\FloatBarrier
\centering 
Fonte:Elaboração própria.

\justifying
\bigskip

Na análise ao tipo de distribuição dos retornos estes não apresentam distribuição normal, sendo que os pressupostos subjacentes a normalidade dos dados não se verificam.

Nos histogramas (figura \@ref(fig:distr)), a linha a azul representa uma distribuição normal (com média 0 e desvio padrão igual ao valor dos dados (retornos log)) de modo a comparar com o histograma, sendo que se verifica distribuição leptocúrticas nos retornos log dos vários índices, como verificado anteriormente.

```{r,distr,echo=FALSE, fig.cap='Distribuição dos retornos',fig.topcaption = TRUE,out.width = "100%"}
par(mfrow  = c(2,2))
hist(returns$TEF.RET,probability=T, main="TEF",xlab="",breaks=100)
lines(density(returns$TEF.RET),col=2)
curve(dnorm(x,0,0.01695272), from = -0.15,to=0.15, col='blue',add = TRUE)
hist(returns$ENEL.RET,probability=T, main="ENEL",xlab="",breaks=100)
lines(density(returns$ENEL.RET),col=2)
curve(dnorm(x,0,0.01571777), from = -0.05,to=0.05, col='blue',add = TRUE)
hist(returns$GLE.RET,probability=T, main="GLE",xlab="",breaks=100)
lines(density(returns$GLE.RET),col=2)
curve(dnorm(x,0,0.02557792), from = -0.1,to=0.1, col='blue',add = TRUE)
hist(returns$AIR.RET,probability=T, main="AIR",xlab="",breaks=100)
lines(density(returns$AIR.RET),col=2)
curve(dnorm(x,0,0.02397011), from = -0.1,to=0.1, col='blue',add = TRUE)
```
\FloatBarrier
\centering 
Fonte:Elaboração própria.

\justifying

Na figura \@ref(fig:teste) "qq plot", verifica-se que nos extremos os dados não se agrupam ao redor da linha reta (negação da normalidade).

```{r,teste,echo=FALSE, fig.cap='Teste a normalidade das distribuições',fig.topcaption = TRUE,out.width = "100%"}
par(mfrow  = c(2,2))
qqnorm(returns$TEF.RET,main="TELEFÓNICA",pch=14,col='darkgreen')
qqline(returns$TEF.RET)
qqnorm(returns$ENEL.RET,main="ENEL",pch=14,col = 'dodgerblue3')
qqline(returns$ENEL.RET)
qqnorm(returns$GLE.RET,main="SOCIÉTÉ GÉNÉRALE",pch=14,col = "darkorange")
qqline(returns$GLE.RET)
qqnorm(returns$AIR.RET,main="AIRBUS",pch=14,col ="deeppink")
qqline(returns$AIR.RET)
```
\FloatBarrier
\centering 
Fonte:Elaboração própria.

\justifying
\bigskip

Por fim, o teste Shapiro-Wilk (teste á normalidade) na tabela \@ref(tab:shapiro) apresentam um valor de p<0.05 (considerando um nível significância a 95%), sendo que se pode rejeitar a distribuição normal dos índices.

\scriptsize
```{r}
#R CODE
lshap<- lapply(as.data.frame(returns), shapiro.test)
lres <- sapply(lshap, "[", c("statistic","p.value"))
```
\normalsize

```{r,shapiro,echo=FALSE}
lresk<- knitr::kable(
  lres, caption = "Teste normalidade Shapiro-Wilk",
  booktabs = TRUE)
kable_styling(lresk , latex_options = c("hold_position","scale_down"), position = "center")
```
\FloatBarrier
\centering 
Fonte:Elaboração própria.

\justifying
\bigskip

\newpage
## EWMA

O *exponential weighted moving average* (EWMA) é calculado com um *decay factor* $\lambda$ de 0.94 e de acordo com a equação \@ref(eq:ewma2). Para esse fim utiliza-se a função do R *ewmaVol* sendo calculado todos os valores de desvio padrão deste o inicio da serie temporal. 

\scriptsize
```{r}
#R CODE
ewma <- function(x) {  
  x<-ewmaVol(x,lambda = 0.94)
  return(tail(x$sigma,1))
}
pewma <- do.call(data.frame, 
                 list("ewma"= round(apply(returns,2,ewma),6)))
```
\normalsize

Na figura \@ref(fig:gewma) podemos ver como acompanha e responde o calculo do EWMA a alterações bruscas da volatilidade diária, verificando-se algum atraso nessa resposta, como esperado, tendo em consideração o próprio método em si e o facto da escolha de um $\lambda$ de 0.94.

```{r,gewma,echo=FALSE, fig.cap='EWMA',fig.topcaption = TRUE,out.width = "80%"}
TEFew <- ewmaVol(returns$TEF.RET,lambda = 0.94)
ENELew <- ewmaVol(returns$ENEL.RET,lambda = 0.94)
GLEew <- ewmaVol(returns$GLE.RET,lambda = 0.94)
AIRew <- ewmaVol(returns$AIR.RET,lambda = 0.94)

par(mfrow  = c(2,2))
TEL1<-plot(returns$TEF.RET,type="l",main="TEF", col="green")
lines(TEFew$sigma,col="darkgreen")
TEL3<-plot(returns$ENEL.RET,type="l",main="ENEL",col="green")
lines(ENELew$sigma,col="dodgerblue3")
TEL5<-plot(returns$GLE.RET,type="l",main="GLE",col="green")
lines(GLEew$sigma,col="darkorange")
TEL7<-plot(returns$AIR.RET,type="l",main="AIR",col="green")
lines(AIRew$sigma,col="deeppink")

```
\FloatBarrier
\centering 
Fonte:Elaboração própria.

\justifying
\bigskip

Na tabela \@ref(tab:ewmaval) temos os valores referentes ao desvio padrão calculado para o último dia de cotação, sendo esse o valor a ser utilizado na simulação.

```{r,ewmaval,echo=FALSE}
pewma<-t(pewma)
pewmak<- knitr::kable(
  pewma, caption = "EWMA",
  booktabs = TRUE)
kable_styling(pewmak , latex_options = "hold_position", position = "center")
```
\FloatBarrier
\centering 
Fonte:Elaboração própria.

\justifying
\bigskip

### Simulação EWMA

O procedimento para a simulação de Monte Carlo será o mesmo para todos os diferentes métodos de calculo de volatilidade, sendo que aqui apenas se apresenta o código R utilizado para a primeira simulação, sendo depois igual para todos as outras situações, onde apenas se altera o valor relativo ao desvio padrão.

A simulação será efectuada para um total de 80 dias de negociação, repetindo 1000 vezes. A equação utilizada para a simulação é a equação \@ref(eq:logprice), em que $\mu$ representa a taxa de juro sem risco referente a obrigações do tesouro alemão a 3 meses^[Germany 3 Month Bond Yield Historical Data - https://www.investing.com/rates-bonds/germany-3-month-bond-yield-historical-data], sendo este o prazo mais próximo do espaço temporal do investimento. 

\scriptsize
```{r,warning=FALSE}
#R CODE
dfewma <- as.data.frame(pewma)

N <- 80
dia <- 1:N
M <- 1000
mu <- -0.00575

TEF.preco_inicial <- TEF.MC$Close[[nrow(TEF.MC$Close)]]
TEF.MC.sigma <- dfewma$TEF.RET*sqrt(252)
TEF.MC.mean <- (mu-(TEF.MC.sigma^2)/2)*(1/252)

set.seed(1)
monte_carlo_sim <- matrix(nrow=N, ncol = M)
for(j in 1:M){
  monte_carlo_sim[[1,j]] <- TEF.preco_inicial
  for(i in 2:N){
    monte_carlo_sim[[i,j]] <- monte_carlo_sim[[i-1,j]]*
      exp(TEF.MC.mean+TEF.MC.sigma *
            rnorm(1,0,1)*sqrt(1/252))
  }
}
TEF.MC_MC_sim <- as_tibble(cbind(dia,monte_carlo_sim))
w <- str_c('Sim',seq(1,M))
w <- c('Dia',w)
names(TEF.MC_MC_sim) <- w
TEF.MC_MC_sim <- gather(TEF.MC_MC_sim, key='Simulation', value = 'Preço',-(Dia))
```
\normalsize

```{r,echo=FALSE, warning=FALSE}
ENEL.preco_inicial <- ENEL.MI$Close[[nrow(ENEL.MI$Close)]]
ENEL.MI.sigma <- dfewma$ENEL.RET*sqrt(252)
ENEL.MI.mean <- (mu-(ENEL.MI.sigma^2)/2)*(1/252)

set.seed(2)
ENEL_monte_carlo_sim <- matrix(nrow=N, ncol = M)
for(j in 1:M){
  ENEL_monte_carlo_sim[[1,j]] <- ENEL.preco_inicial
  for(i in 2:N){
    ENEL_monte_carlo_sim[[i,j]] <- ENEL_monte_carlo_sim[[i-1,j]]*exp(ENEL.MI.mean+ENEL.MI.sigma * rnorm(1,0,1)*sqrt(1/252))
  }
}
ENEL.MI_MC_sim <- as_tibble(cbind(dia,ENEL_monte_carlo_sim))
w1 <- str_c('Sim',seq(1,M))
w1 <- c('Dia',w1)
names(ENEL.MI_MC_sim) <- w1
ENEL.MI_MC_sim <- gather(ENEL.MI_MC_sim, key='Simulation', value = 'Preço',-(Dia))
```

```{r,echo=FALSE, warning=FALSE}
GLE.preco_inicial <- GLE.PA$Close[[nrow(GLE.PA$Close)]]
GLE.PA.sigma <- dfewma$GLE.RET*sqrt(252)
GLE.PA.mean <- (mu-(GLE.PA.sigma^2)/2)*(1/252)

set.seed(3)
GLE_monte_carlo_sim <- matrix(nrow=N, ncol = M)
for(j in 1:M){
  GLE_monte_carlo_sim[[1,j]] <- GLE.preco_inicial
  for(i in 2:N){
    GLE_monte_carlo_sim[[i,j]] <- GLE_monte_carlo_sim[[i-1,j]]*exp(GLE.PA.mean+GLE.PA.sigma * rnorm(1,0,1)*sqrt(1/252))
  }
}
GLE.PA_MC_sim <- as_tibble(cbind(dia,GLE_monte_carlo_sim))
w2 <- str_c('Sim',seq(1,M))
w2 <- c('Dia',w2)
names(GLE.PA_MC_sim) <- w2
GLE.PA_MC_sim <- gather(GLE.PA_MC_sim, key='Simulation', value = 'Preço',-(Dia))
```

```{r,echo=FALSE, warning=FALSE}
AIR.preco_inicial <- AIR.PA$Close[[nrow(AIR.PA$Close)]]
AIR.PA.sigma <- dfewma$AIR.RET*sqrt(252)
AIR.PA.mean <- (mu-(AIR.PA.sigma^2)/2)*(1/252)

set.seed(4)
AIR_monte_carlo_sim <- matrix(nrow=N, ncol = M)
for(j in 1:M){
  AIR_monte_carlo_sim[[1,j]] <- AIR.preco_inicial
  for(i in 2:N){
    AIR_monte_carlo_sim[[i,j]] <- AIR_monte_carlo_sim[[i-1,j]]*exp(AIR.PA.mean+AIR.PA.sigma * rnorm(1,0,1)*sqrt(1/252))
  }
}
AIR.PA_MC_sim <- as_tibble(cbind(dia,AIR_monte_carlo_sim))
w3 <- str_c('Sim',seq(1,M))
w3 <- c('Dia',w3)
names(AIR.PA_MC_sim) <- w3
AIR.PA_MC_sim <- gather(AIR.PA_MC_sim, key='Simulation', value = 'Preço',-(Dia))
```

Na figura \@ref(fig:sim) encontram-se as simulações efectuadas para cada uma das empresas.

```{r,sim,echo=FALSE, fig.cap='Simulação Monte Carlo para 80 dias',fig.topcaption = TRUE,out.width = "45%"}

par(mfrow  = c(2,2))
ggplot(TEF.MC_MC_sim,aes(x = Dia, y = Preço, Group = Simulation)) + geom_line(alpha = 0.1,colour = 'darkgreen'
) + ggtitle(str_c("TEF: ", M, " Simulações de Monte Carlo ao longo de ", N," dias negociação"))
ggplot(ENEL.MI_MC_sim,aes(x = Dia, y = Preço, Group = Simulation)) + geom_line(alpha = 0.1,colour = 'dodgerblue3') + ggtitle(str_c("ENEL: ", M, " Simulações de Monte Carlo ao longo de ", N," dias negociação"))
ggplot(GLE.PA_MC_sim,aes(x = Dia, y = Preço, Group = Simulation)) + geom_line(alpha = 0.1,colour = "darkorange"
) + ggtitle(str_c("GLE: ", M, " Simulações de Monte Carlo ao longo de ", N," dias negociação"))
ggplot(AIR.PA_MC_sim,aes(x = Dia, y = Preço, Group = Simulation)) + geom_line(alpha = 0.1,colour="deeppink") + ggtitle(str_c("AIR: ", M, " Simulações de Monte Carlo ao longo de ", N," dias negociação"))
```

A partir dos valores simulados calcula-se a média para cada um dos dias de acordo com o código R abaixo, obtendo-se desta forma os dados ilustrados na tabela \@ref(tab:simtabewma) referente aos últimos 5 dias.

\scriptsize
```{r}
#R CODE
M.TEF.MC <- apply(monte_carlo_sim, 1, mean)
M.ENEL.MI <- apply(ENEL_monte_carlo_sim, 1,mean )
M.GLE.PA <- apply(GLE_monte_carlo_sim, 1,mean )
M.AIR.PA <- apply(AIR_monte_carlo_sim, 1,mean )
```
\normalsize

```{r,simtabewma,echo=FALSE}
SimEWMA <- as.data.frame(cbind(M.TEF.MC,M.ENEL.MI,M.GLE.PA,M.AIR.PA))
SimEWMAk<- knitr::kable(
  tail(SimEWMA), caption = "EWMA - Valores simulados",
  booktabs = TRUE)
kable_styling(SimEWMAk , latex_options = "hold_position", position = "center")
```
\FloatBarrier
\centering 
Fonte:Elaboração própria.

\justifying
\bigskip

### Portfolio EWMA

```{r,echo=FALSE}
SimDate<-read_xlsx ("data/date.xlsx")
SDate<- as.Date(SimDate$Date)
SimTEF <- as.numeric(SimEWMA$M.TEF.MC)
SimTEF.MC <-xts(SimTEF,SDate)
SimENEL <- as.numeric(SimEWMA$M.ENEL.MI)
SimENEL.MI <-xts(SimENEL,SDate)
SimGLE <- as.numeric(SimEWMA$M.GLE.PA)
SimGLE.PA <-xts(SimGLE,SDate)
SimAIR <- as.numeric(SimEWMA$M.AIR.PA)
SimAIR.PA <-xts(SimAIR,SDate)

myPortfolio <-na.omit(merge(SimTEF.MC,SimENEL.MI,SimGLE.PA,SimAIR.PA))

myPortfolioReturns <- ROC(myPortfolio,type="discrete")[-1,]
colnames(myPortfolioReturns) <- c("SimTEF","SimENEL","SimGLE","SimAIR")
```

Na definição do portfolio vamos utilizar os dados referentes às simulações realizadas, considerando que o objectivo é minimizar o risco, ou seja minimizar a variância. A função utilizada pelo R é *minvariancePortfolio*, sendo desta forma definido os pesos de cada um dos activos que irão constituir o portfolio, apresentados na tabela \@ref(tab:pewma).

\scriptsize
```{r}
# R CODE
SimReturns <- as.timeSeries(myPortfolioReturns)
FronteiraEff <- portfolioFrontier(SimReturns, constraints = "LongOnly")

minvarewma <- minvariancePortfolio(SimReturns) 
minew<-getPortfolio(minvarewma)
```
\normalsize

A figura \@ref(fig:frontewma) representa o gráfico referente a fronteira eficiente para o portfolio de empresas em análise, sendo que o ponto a vermelho é onde se obtêm a mínima variância ou risco.

```{r,frontewma,echo=FALSE, fig.cap='EWMA - Fronteira eficiente',fig.topcaption = TRUE,out.width = "80%"}
plot(FronteiraEff,c(1,2,3,4))
```
\FloatBarrier
\centering 
Fonte:Elaboração própria.

\justifying
\bigskip

Como se pode ver a empresa ENEL será a empresa com maior peso no portfolio com cerca de 55%, seguida pela TELEFONICA com cerca de 25%. As outras terão uma ponderação mais residual.

```{r,pewma,echo=FALSE}
minewma<- as.data.frame(minew$weights)
colnames(minewma)<- c("Pesos")
minewmak<- knitr::kable(
  minewma, caption = "EWMA - Pesos no portfolio",
  booktabs = TRUE)
kable_styling(minewmak , latex_options = "hold_position", position = "center")
```
\FloatBarrier
\centering 
Fonte:Elaboração própria.

\justifying
\bigskip

Definidos as ponderações de cada empresa no portfolio podemos calcula o VaR assim como as contribuições individuais de cada um dos activos, podendo-se verificar nos dados abaixo que a maior contribuição é dada pela ENEL, sendo natural, pois apresenta um grande peso no portfolio. No cálculo do VaR foi utilizado o VaR paramétrico com aproximação Cornish-Fisher, pois é mais representativo da realidade da distribuição dos retornos dos activos financeiros.

\scriptsize
```{r}
#R CODE
ewmaweights <- c(0.2502928, 0.5540359, 0.1141192, 0.0815521)
ModVaR_95_ewma <- VaR(myPortfolioReturns, p=0.95,weights=ewmaweights, 
                      portfolio_method = "component", method = "modified")
ModVaR_95_ewma$contribution 
```
\normalsize

```{r,echo=FALSE}
MVaRewma_VaR <- round(ModVaR_95_ewma$MVaR,6)
```

Analisando o valor calculado para a portfolio, pode-se verificar que para um nível de confiança 95%, o VaR a 1 dia para dados simulados a 80 dias é de `r MVaRewma_VaR`, ou seja, existe uma probabilidade de 5% de ocorrer uma perda igual ou superior a este valor no montante total do portfolio investido.

## GARCH

A aplicação do modelo GARCH para estimar a volatilidade será aplicado de acordo com a verificação dos pressupostos e da estimação dos parâmetros, sendo inicialmente utilizado o modelo genérico GARCH(1,1), e só caso não se verifique que o modelo responde adequadamente é que se tentará analisar a volatilidade com outros modelos, como o ARMA-GARCH e o EGARCH, optando pelo modelo que demonstre se adequar melhor aos dados. As tabelas relativas aos *outputs* dos modelos encontram-se no apêndice I.

Nos modelos utilizados pelo pacotes rugarch e fGARCH o LM-ARCH teste é um teste *portmanteau* ponderado onde se testa a hipótese nula do processo ARCH estar adequadamente ajustado, enquanto o teste Ljung-Box é outro *portmanteau* teste que testa a hipótese nula da adequação do ajuste ARMA.

### TELEFÓNICA

A aplicação do modelo é realizado com um modelo GARCH de primeira ordem, aplicando-se uma distribuição condicional dos retornos standardizados do tipo *t de Student*, sendo que o teste Jarque-Bera e o Shapiro-Wilk rejeitam para um nível de significância de 5% a normalidade dos dados, pois o valor *p-value* é inferior ao nível de significância, rejeitando-se deste modo $H_0$, ou seja, que os dados apresentam uma distribuição normal (ver Apêndice). A escolha deste tipo de distribuição prendem-se com o facto dos retornos apresentarem uma densidade do tipo *leptokurtic*.(@rvol)

A estimação do modelo GARCH(1,1) obteve uma boa resposta na modelação da volatilidade, sendo que todos os parâmetros do modelo, nomeadamente o $\omega$, o $\alpha$ e o $\beta$ apresentam *p-values* extremamente fortes, sendo significantes a um nível inferior a 5%, evidenciando que o modelo com os parâmetros apresentados providenciam o melhor ajuste aos dados, como podemos ver na tabela \@ref(tab:tefcoefgarch).

\scriptsize
```{r,warning = FALSE}
#R CODE
TEF<-garchFit(formula = ~ garch(1,1),data=returns$TEF.RET,cond.dist = "std",  include.mean=FALSE, 
              trace=FALSE)
```
\normalsize

```{r,tefcoefgarch, echo=FALSE}
TEFcoef <- round(TEF@fit$matcoef,6)

TEFcoefk<- knitr::kable(
  TEFcoef, caption = "TEF coeficientes",
  booktabs = TRUE)
kable_styling(TEFcoefk , latex_options = "hold_position", position = "center")
```
\FloatBarrier
\centering 
Fonte:Elaboração própria.

\justifying
\bigskip

```{r, echo=FALSE}
pers <- TEF@fit$params$persistence
```
o valor referente a persistência apresenta um valor de `r pers` sendo inferior a 1, apresentando deste modo uma regressão à média, ou seja que os valores referentes à volatilidade vão eventualmente regressar ao valores médios a longo prazo. 

O modelo também disponibiliza os dados referentes aos critérios de informação estatísticos, como o critério de informação Akaike (AIC), sendo um modelo que estima a quantidade de informação perdida para um determinado modelo. Estes indicadores são adequados na comparação de vários modelos, sendo que quanto menor o valor deste indicador melhor o modelo. Neste caso este indicador foi utilizado sempre que se verificou que o modelo não apresentava uma boa resposta, comparando-se os vários modelos alternativos. A informação relativa a estes indicadores encontra-se no apêndice I.

O modelo também disponibiliza informação relativa aos resíduos estandardizados para verificação de auto-correlação. De acordo com (@rvol) " se a evidência sugerir auto-correlação nos resíduos estandardizados, então o modelo GARCH(1,1) não capturou toda a dinâmica da volatilidade dos retornos". 

Na tabela \@ref(tab:TEFLjunk) podemos verificar que as estatísticas para o teste Ljung-Box demonstram que o modelo integrou a volatilidade dos retornos, sendo que não se pode rejeitar a hipótese nula de que não existe auto-correlação entre os resíduos estandardizados, ou seja, as especificações são adequadas para capturar a auto-correlação e a variação da volatilidade no tempo nas series dos retornos.

\scriptsize
```{r}
#R CODE
TEFr <-residuals(TEF, standardize = T)
TEFres <- as.data.frame(LjungBox(TEFr, lags=c(10,15,20), order=0, season=1, squared.residuals=FALSE))
TEFsre <- as.data.frame(LjungBox(TEFr^2, lags=c(10,15,20), order=0, season=1, squared.residuals=FALSE))
```
\normalsize

```{r,TEFLjunk,echo=FALSE}
TEFLjun <- rbind(TEFres,TEFsre)
rownames(TEFLjun)<-c( "Ljung-Box Test 10","Ljung-Box Test 15","Ljung-Box Test 20","Ljung-Box Test 10 R^2 ","Ljung-Box Test 15 R^2","Ljung-Box Test 20 R^2")
TEFLjun$df<-NULL

TEFLjunk<- knitr::kable(
  TEFLjun, caption = "Standardized Residuals Tests",
  booktabs = TRUE)
kable_styling(TEFLjunk , latex_options = "hold_position", position = "center")
```
\FloatBarrier
\centering 
Fonte:Elaboração própria.

\justifying
\bigskip

Na figura \@ref(fig:TELplot) temos representados 4 gráficos, um com o desvio padrão estimado pelo modelo, outro com o ajuste da volatilidade estimada com um intervalo de confiança para 95%, assim como o gráfico relativo à função de auto-correlação (ACF) para os resíduos estandardizados e os resíduos estandardizados ao quadrado, sendo que se verifica que a auto-correlação se encontra próximo de 0. De acordo com @foregeorge, "Se um ou mais picos grandes estiverem fora desses limites, ou se substancialmente mais de 5% dos picos estiverem fora desses limites, a série provavelmente não é *white noise*". Estes gráficos apenas vêm confirmar o que já tinha sido verificado com o teste Ljung-Box.
```{r,TELplot,echo=FALSE, fig.cap='Telefónica',fig.topcaption = TRUE,out.width = "80%"}
par(mfrow  = c(2,2))
plot(TEF,which= 2)
plot(TEF,which= 3)
plot(TEF,which=4)
plot(TEF,which=5)
```
\FloatBarrier
\centering 
Fonte:Elaboração própria.

\justifying
\bigskip

```{r,echo=FALSE}
TEFsd <- TEF@sigma.t
TEFvol <- round(tail(TEFsd,1),6)
```
O valor de desvio padrão diário que vamos utilizar para efeitos de simulação, é o valor de `r TEFvol`, sendo este o valor representativo da volatilidade condicional no último dia de cotação.

### ENEL

A aplicação do modelo neste caso continua a ser realizado com um modelo GARCH de primeira ordem, aplicando-se uma distribuição condicional dos retornos standardizados do tipo *t de Student*, sendo que o teste Jarque-Bera e o Shapiro-Wilk rejeitam para um nível de significância de 5% a normalidade dos dados, pois o valor *p-value* é inferior ao nível de significância, rejeitando-se deste modo $H_0$, ou seja, que os dados apresentam uma distribuição normal (ver Apêndice).

A estimação do modelo GARCH(1,1) obteve uma boa resposta na modelação da volatilidade, sendo que todos os parâmetros do modelo, nomeadamente o $\omega$, o $\alpha$ e o $\beta$ apresentam *p-values* extremamente fortes, sendo significantes a um nível inferior a 5%, evidenciando que o modelo com os parâmetros apresentados providenciam o melhor ajuste aos dados, como podemos ver na tabela \@ref(tab:ENELcoef).

\scriptsize
```{r,  warning = FALSE}
#R CODE
ENEL<-garchFit(formula = ~ garch(1,1),data=returns$ENEL.RET,cond.dist = "std", include.mean=FALSE, 
               trace=FALSE)
```
\normalsize

```{r,ENELcoef, echo=FALSE}
ENELcoef <- round(ENEL@fit$matcoef,6)

ENELcoefk<- knitr::kable(
  ENELcoef, caption = "ENEL coeficientes",
  booktabs = TRUE)
kable_styling(ENELcoefk , latex_options = "hold_position", position = "center")
```
\FloatBarrier
\centering 
Fonte:Elaboração própria.

\justifying
\bigskip

```{r,ENELpers,echo=FALSE}
ENELpers <- ENEL@fit$params$persistence
```
o valor referente a persistência apresenta um valor de `r ENELpers` sendo inferior a 1, apresentando deste modo uma regressão à média e um processo GARCH(1,1) estável.

Os dados referentes aos critérios de informação estatísticos, como o critério de informação Akaike (AIC), apresentam valores bastante baixos.

Na tabela \@ref(tab:ENELLjunk) podemos verificar que as estatísticas para o teste Ljung-Box demonstram que o modelo integrou a volatilidade dos retornos, sendo que não se pode rejeitar a hipótese nula de que não existe auto-correlação entre os resíduos estandardizados, ou seja, as especificações são adequadas para capturar a auto-correlação e a variação da volatilidade no tempo nas series dos retornos.

```{r,ENELLjunk, echo=FALSE}
ENELr <-residuals(ENEL, standardize = T)
ENELres <- as.data.frame(LjungBox(ENELr, lags=c(10,15,20), order=0, season=1, squared.residuals=FALSE))
ENELsre <- as.data.frame(LjungBox(ENELr^2, lags=c(10,15,20), order=0, season=1, squared.residuals=FALSE))
ENELLjun <- rbind(ENELres,ENELsre)
rownames(ENELLjun)<-c( "Ljung-Box Test 10","Ljung-Box Test 15","Ljung-Box Test 20","Ljung-Box Test 10 R^2 ","Ljung-Box Test 15 R^2","Ljung-Box Test 20 R^2")
ENELLjun$df<-NULL

ENELLjunk<- knitr::kable(
  ENELLjun, caption = "ENEL Standardized Residuals Tests",
  booktabs = TRUE)
kable_styling(ENELLjunk , latex_options = "hold_position", position = "center")
```
\FloatBarrier
\centering 
Fonte:Elaboração própria.

Na figura \@ref(fig:ENELplot) temos representados 4 gráficos, um com o desvio padrão estimado pelo modelo, outro com o ajuste da volatilidade estimada com um intervalo de confiança para 95%, assim como o gráfico relativo à função de auto-correlação (ACF) para os resíduos estandardizados e os resíduos estandardizados ao quadrado, sendo que se verifica que a auto-correlação se encontra próximo de 0. Estes gráficos apenas vêm confirmar o que já tinha sido verificado com o teste Ljung-Box.

\justifying
\bigskip
```{r,ENELplot,echo=FALSE, fig.cap='ENEL',fig.topcaption = TRUE,out.width = "100%"}
par(mfrow  = c(2,2))
plot(ENEL,which= 2)
plot(ENEL,which= 3)
plot(ENEL,which=4)
plot(ENEL,which=5)
```
\FloatBarrier
\centering 
Fonte:Elaboração própria.

\justifying
\bigskip

```{r,echo=FALSE}
ENELsd <- ENEL@sigma.t
ENELvol <- round(tail(ENELsd,1),6)
```

O valor de desvio padrão diário que vamos utilizar para efeitos de simulação, é o valor de `r ENELvol`, sendo este o valor representativo da volatilidade condicional no último dia de cotação.

### SOCIÉTÉ GÉNÉRALE

A aplicação do modelo neste caso contemplou também o modelo AR(1) de modo a que o modelo respondesse melhor a auto-correlação, sendo que ao utilizar apenas o modelo GARCH(1,1) o teste Ljung-Box rejeitava a hipótese nula de que as autocorrelações até lag k são iguais a zero, para um nível de significância de 5%. 

No modelo GARCH(1,1) continuou-se a aplicar uma distribuição condicional dos retornos standardizados do tipo *t de Student*, sendo que o teste Jarque-Bera e o Shapiro-Wilk rejeitam para um nível de significância de 5% a normalidade dos dados.

Ao realizar o ajuste do modelo GARCH(1,1) com o modelo AR(1) a estimação do modelo obteve uma boa resposta na modelação da volatilidade, sendo que todos os parâmetros do modelo, nomeadamente o $\alpha$ e o $\beta$ apresentam *p-values* extremamente fortes, sendo significantes a um nível inferior a 5%, evidenciando que o modelo com os parâmetros apresentados providenciam um bom ajuste aos dados, como podemos ver na tabela \@ref(tab:EGLEcoefk).

\scriptsize
```{r,warning = FALSE}
GLE<-garchFit(formula = ~ arma(1,0) + garch(1,1),data=returns$GLE.RET,cond.dist = "std",include.mean=FALSE, trace=FALSE)
```
\normalsize

```{r,GLEcoefk,echo=FALSE}
GLEcoef <- round(GLE@fit$matcoef,6)

GLEcoefk<- knitr::kable(
  GLEcoef, caption = "GLE coeficientes",
  booktabs = TRUE)
kable_styling(GLEcoefk , latex_options = "hold_position", position = "center")
```
\FloatBarrier
\centering 
Fonte:Elaboração própria.

\justifying
\bigskip

```{r,echo=FALSE}
GLEpers <- GLE@fit$params$persistence
```
o valor referente a persistência apresenta um valor de `r GLEpers` sendo inferior a 1, apresentando deste modo uma regressão à média e um processo GARCH(1,1) estável.

Os dados referentes aos critérios de informação estatísticos, como o critério de informação Akaike (AIC), foram analisados no comparativo com vários modelos, sendo que este modelo apresentava valores bastante baixos.

Na tabela \@ref(tab:GLELjunk) podemos verificar que as estatísticas para o teste Ljung-Box demonstram que o modelo integrou a volatilidade dos retornos, sendo que não se pode rejeitar a hipótese nula de que não existe auto-correlação entre os resíduos estandardizados, ou seja, as especificações são adequadas para capturar a auto-correlação e a variação da volatilidade no tempo nas series dos retornos.

```{r,GLELjunk,echo=FALSE}
GLEr <-residuals(GLE, standardize = T)
GLEres <- as.data.frame(LjungBox(GLEr, lags=c(10,15,20), order=0, season=1, squared.residuals=FALSE))
GLEsre <- as.data.frame(LjungBox(GLEr^2, lags=c(10,15,20), order=0, season=1, squared.residuals=FALSE))
GLELjun <- rbind(GLEres,GLEsre)
rownames(GLELjun)<-c( "Ljung-Box Test 10","Ljung-Box Test 15","Ljung-Box Test 20","Ljung-Box Test 10 R^2 ","Ljung-Box Test 15 R^2","Ljung-Box Test 20 R^2")
GLELjun$df<-NULL

GLELjunk<- knitr::kable(
  GLELjun, caption = "GLE Standardized Residuals Tests",
  booktabs = TRUE)
kable_styling(GLELjunk , latex_options = "hold_position", position = "center")
```
\FloatBarrier
\centering 
Fonte:Elaboração própria.

\justifying
\bigskip

Do mesmo na figura \@ref{fig:GLEplot} podemos visualizar o gráfico referente aos desvios padrão calculados pelo modelo, assim como a volatilidade para um intervalo de confiança a 95%, verificando-se uma boa resposta por parte do modelo.

Os gráficos ACF vêm complementar a informação relativa aos teste Ljung-Box.

```{r,GLEplot,echo=FALSE, fig.cap='SOCIÉTÉ GÉNÉRALE',fig.topcaption = TRUE,out.width = "100%"}
par(mfrow  = c(2,2))
plot(GLE,which= 2)
plot(GLE,which= 3)
plot(GLE,which=4)
plot(GLE,which=5)
```
\FloatBarrier
\centering 
Fonte:Elaboração própria.

\justifying
\bigskip

```{r,echo=FALSE}
GLEsd <- GLE@sigma.t
GLEvol <- round(tail(GLEsd,1),6)
```

O valor de desvio padrão diário que vamos utilizar para efeitos de simulação, é o valor de `r GLEvol`, sendo este o valor representativo da volatilidade condicional no último dia de cotação.

### AIRBUS

O modelo aplicado aos retornos da empresa Airbus foi o EGARCH(2,2), conjuntamente com o ARIMA(1,1), sendo este que apresentava melhor reposta ao modelo ajustado. A tentativa de aplicação de outros modelos como o  GARCH(1,1) não permitia extrair nenhum parâmetro do modelo que apresenta-se significância a um nível inferior a 5%. Outros modelos com ordem de 1 falhavam no teste ARCH, embora no teste Ljung-Box apresentassem uma resposta razoável.

Assim, aceita-se o modelo ARIMA(1,1)-EGARCH(2,2), sendo que se conseguem extrair vários parâmetros significantes a um nível inferior a 5%, de acordo com a tabela @ref(tab:AIRcoefk).

\scriptsize
```{r,warning = FALSE}
#R CODE
AIR.spec <- ugarchspec(variance.model=list(model="eGARCH",garchOrder=c(2,2)),
                           mean.model=list(armaOrder=c(1,1)),distribution.model="std")

AIR <- ugarchfit(AIR.spec, returns$AIR.RET) 
```
\normalsize

```{r,AIRcoefk, echo=FALSE}
AIRcoef <- round(AIR@fit$matcoef,6)

AIRcoefk<- knitr::kable(
  AIRcoef, caption = "AIR coeficientes",
  booktabs = TRUE)
kable_styling(AIRcoefk , latex_options = "hold_position", position = "center")
```
\FloatBarrier
\centering 
Fonte:Elaboração própria.

\justifying
\bigskip

```{r,echo=FALSE}
AIRpers <- AIR@fit$persistence
```
o valor referente a persistência apresenta um valor de `r AIRpers` sendo inferior a 1, apresentando deste modo uma regressão à média e um processo estável.

Os dados referentes aos critérios de informação estatísticos, como o critério de informação Akaike (AIC), foram analisados no comparativo com vários modelos, sendo que este modelo apresentava valores bastante baixos.

Na tabela \@ref(tab:AIRLjunk) podemos verificar que as estatísticas para o teste Ljung-Box demonstram que o modelo integrou a volatilidade dos retornos. No apêndice I, na tabela referente ao *output* do modelo o teste referente ao LM-ARCH também podemos confirmar que os resíduos estandardizados se comportam como um *white noise*, não se podendo rejeitar a hipótese nula visto não haver evidência de auto-correlação.

```{r,AIRLjunk, echo=FALSE}
AIRr <-as.numeric(residuals(AIR, standardize = T))
AIRres <-as.matrix(LjungBox(AIRr, lags=c(10,15,20), order=0, season=1, squared.residuals=FALSE))
AIRsre <- as.data.frame(LjungBox(AIRr, lags=c(10,15,20), order=0, season=1, squared.residuals=TRUE))
AIRLjun <- rbind(AIRres,AIRsre)
rownames(AIRLjun)<-c( "Ljung-Box Test 10","Ljung-Box Test 15","Ljung-Box Test 20","Ljung-Box Test 10 R^2 ","Ljung-Box Test 15 R^2","Ljung-Box Test 20 R^2")
AIRLjun$df<-NULL

AIRLjunk<- knitr::kable(
  AIRLjun, caption = "AIR Standardized Residuals Tests",
  booktabs = TRUE)
kable_styling(AIRLjunk , latex_options = "hold_position", position = "center")
```
\FloatBarrier
\centering 
Fonte:Elaboração própria.

\justifying
\bigskip

Na figura \@ref{fig:AIRplot} podemos visualizar o gráfico referente aos desvios padrão calculados pelo modelo, assim como a volatilidade para um intervalo de confiança a 95%, verificando-se uma boa resposta por parte do modelo.

Os gráficos ACF vêm complementar a informação relativa aos teste Ljung-Box.
```{r,AIRplot,echo=FALSE, fig.cap='AIRBUS',fig.topcaption = TRUE,out.width = "100%"}
par(mfrow  = c(2,2))
plot(AIR,which= 1)
plot(AIR,which= 3)
plot(AIR,which=10)
plot(AIR,which=11)
```
\FloatBarrier
\centering 
Fonte:Elaboração própria.

\justifying
\bigskip

```{r,echo=FALSE}
AIRsd <- AIR@fit$sigma
AIRvol <- round(tail(AIRsd,1),6)
```

O valor de desvio padrão diário que vamos utilizar para efeitos de simulação, é o valor de `r AIRvol`, sendo este o valor representativo da volatilidade condicional no último dia de cotação.

### Simulação GARCH

```{r,echo=FALSE, warning=FALSE}
TEF.preco_inicial <- TEF.MC$Close[[nrow(TEF.MC$Close)]]
TEF.MC.sigma <- TEFvol*sqrt(252)
TEF.MC.mean <- (mu-(TEF.MC.sigma^2)/2)*(1/252)

set.seed(5)
monte_carlo_sim <- matrix(nrow=N, ncol = M)
for(j in 1:M){
  monte_carlo_sim[[1,j]] <- TEF.preco_inicial
  for(i in 2:N){
    monte_carlo_sim[[i,j]] <- monte_carlo_sim[[i-1,j]]*
      exp(TEF.MC.mean+TEF.MC.sigma *
            rnorm(1,0,1)*sqrt(1/252))
  }
}

TEF.MC_MC_sim <- as_tibble(cbind(dia,monte_carlo_sim))
w <- str_c('Sim',seq(1,M))
w <- c('Dia',w)
names(TEF.MC_MC_sim) <- w
TEF.MC_MC_sim <- gather(TEF.MC_MC_sim, key='Simulation', value = 'Preço',-(Dia))
```


```{r,echo=FALSE, warning=FALSE}
ENEL.preco_inicial <- ENEL.MI$Close[[nrow(ENEL.MI$Close)]]
ENEL.MI.sigma <- ENELvol*sqrt(252)
ENEL.MI.mean <- (mu-(ENEL.MI.sigma^2)/2)*(1/252)

set.seed(6)
ENEL_monte_carlo_sim <- matrix(nrow=N, ncol = M)
for(j in 1:M){
  ENEL_monte_carlo_sim[[1,j]] <- ENEL.preco_inicial
  for(i in 2:N){
    ENEL_monte_carlo_sim[[i,j]] <- ENEL_monte_carlo_sim[[i-1,j]]*exp(ENEL.MI.mean+ENEL.MI.sigma * rnorm(1,0,1)*sqrt(1/252))
  }
}
ENEL.MI_MC_sim <- as_tibble(cbind(dia,ENEL_monte_carlo_sim))
w1 <- str_c('Sim',seq(1,M))
w1 <- c('Dia',w1)
names(ENEL.MI_MC_sim) <- w1
ENEL.MI_MC_sim <- gather(ENEL.MI_MC_sim, key='Simulation', value = 'Preço',-(Dia))
```

```{r,echo=FALSE, warning=FALSE}
GLE.preco_inicial <- GLE.PA$Close[[nrow(GLE.PA$Close)]]
GLE.PA.sigma <- GLEvol*sqrt(252)
GLE.PA.mean <- (mu-(GLE.PA.sigma^2)/2)*(1/252)

set.seed(7)
GLE_monte_carlo_sim <- matrix(nrow=N, ncol = M)
for(j in 1:M){
  GLE_monte_carlo_sim[[1,j]] <- GLE.preco_inicial
  for(i in 2:N){
    GLE_monte_carlo_sim[[i,j]] <- GLE_monte_carlo_sim[[i-1,j]]*exp(GLE.PA.mean+GLE.PA.sigma * rnorm(1,0,1)*sqrt(1/252))
  }
}
GLE.PA_MC_sim <- as_tibble(cbind(dia,GLE_monte_carlo_sim))
w2 <- str_c('Sim',seq(1,M))
w2 <- c('Dia',w2)
names(GLE.PA_MC_sim) <- w2
GLE.PA_MC_sim <- gather(GLE.PA_MC_sim, key='Simulation', value = 'Preço',-(Dia))
```

```{r,echo=FALSE, warning=FALSE}
AIR.preco_inicial <- AIR.PA$Close[[nrow(AIR.PA$Close)]]
AIR.PA.sigma <- AIRvol*sqrt(252)
AIR.PA.mean <- (mu-(AIR.PA.sigma^2)/2)*(1/252)

set.seed(8)
AIR_monte_carlo_sim <- matrix(nrow=N, ncol = M)
for(j in 1:M){
  AIR_monte_carlo_sim[[1,j]] <- AIR.preco_inicial
  for(i in 2:N){
    AIR_monte_carlo_sim[[i,j]] <- AIR_monte_carlo_sim[[i-1,j]]*exp(AIR.PA.mean+AIR.PA.sigma * rnorm(1,0,1)*sqrt(1/252))
  }
}
AIR.PA_MC_sim <- as_tibble(cbind(dia,AIR_monte_carlo_sim))
w3 <- str_c('Sim',seq(1,M))
w3 <- c('Dia',w3)
names(AIR.PA_MC_sim) <- w3
AIR.PA_MC_sim <- gather(AIR.PA_MC_sim, key='Simulation', value = 'Preço',-(Dia))
```

A simulação será efectuada tendo em consideração os mesmos parâmetros utilizados pelo modelo EWMA, excepto para os valores calculados para o desvio padrão no último dia de cotação, sendo esses valores integrados no modelo.

Na figura \@ref(fig:simgarch) temos a simulação efectuada 1000 vezes ao logo de 80 dias para as 4 empresas em análise.

```{r,simgarch,echo=FALSE, fig.cap='Simulação Monte Carlo para 80 dias',fig.topcaption = TRUE,out.width = "45%"}
par(mfrow  = c(2,2))
ggplot(TEF.MC_MC_sim,aes(x = Dia, y = Preço, Group = Simulation)) + geom_line(alpha = 0.1,colour = 'darkgreen'
) + ggtitle(str_c("TEF: ", M, " Simulações de Monte Carlo ao longo de ", N," dias negociação"))
ggplot(ENEL.MI_MC_sim,aes(x = Dia, y = Preço, Group = Simulation)) + geom_line(alpha = 0.1,colour = 'dodgerblue3'
) + ggtitle(str_c("ENEL: ", M, " Simulações de Monte Carlo ao longo de ", N," dias negociação"))
ggplot(GLE.PA_MC_sim,aes(x = Dia, y = Preço, Group = Simulation)) + geom_line(alpha = 0.1,colour = "darkorange"
) + ggtitle(str_c("GLE: ", M, " Simulações de Monte Carlo ao longo de ", N," dias negociação"))
ggplot(AIR.PA_MC_sim,aes(x = Dia, y = Preço, Group = Simulation)) + geom_line(alpha = 0.1,colour="deeppink") + ggtitle(str_c("AIR: ", M, " Simulações de Monte Carlo ao longo de ", N," dias negociação"))
```


```{r}
M.TEF.MC <- apply(monte_carlo_sim, 1, mean)
M.ENEL.MI <- apply(ENEL_monte_carlo_sim, 1,mean )
M.GLE.PA <- apply(GLE_monte_carlo_sim, 1,mean )
M.AIR.PA <- apply(AIR_monte_carlo_sim, 1,mean )
```

A partir dos valores simulados calcula-se a média para cada um dos dias obtendo-se desta forma os dados ilustrados na tabela \@ref(tab:simtabgarch) referente aos últimos 5 dias.

```{r,simtabgarch,echo=FALSE}
SimGARCH <- as.data.frame(cbind(M.TEF.MC,M.ENEL.MI,M.GLE.PA,M.AIR.PA))
SimGARCHk<- knitr::kable(
  tail(SimGARCH), caption = "GARCH - Valores simulados",
  booktabs = TRUE)
kable_styling(SimGARCHk , latex_options = "hold_position", position = "center")
```
\FloatBarrier
\centering 
Fonte:Elaboração própria.

\justifying
\bigskip

### Portfolio GARCH

Da mesma forma que realizado aquando da definição do portfólio para o modelo EWMA, vamos utilizar os dados referentes às simulações realizadas, aplicando a mesma função R, com o objectivo de apresentar o portfolio que minimize a variância total. Os valores apurados pra os pesos de cada um dos activos que irão integrar o portfolio encontram-se na tabela \@ref(tab:pgarchw).

```{r,echo=FALSE}
SimDate<-read_xlsx ("data/date.xlsx")
SDate<- as.Date(SimDate$Date)
SimTEF <- as.numeric(SimGARCH$M.TEF.MC)
SimTEF.MC <-xts(SimTEF,SDate)
SimENEL <- as.numeric(SimGARCH$M.ENEL.MI)
SimENEL.MI <-xts(SimENEL,SDate)
SimGLE <- as.numeric(SimGARCH$M.GLE.PA)
SimGLE.PA <-xts(SimGLE,SDate)
SimAIR <- as.numeric(SimGARCH$M.AIR.PA)
SimAIR.PA <-xts(SimAIR,SDate)

myPortfolio <-na.omit(merge(SimTEF.MC,SimENEL.MI,SimGLE.PA,SimAIR.PA))

myPortfolioReturns <- ROC(myPortfolio,type="discrete")[-1,]
colnames(myPortfolioReturns) <- c("SimTEF","SimENEL","SimGLE","SimAIR")
```

```{r}
SimReturnsG <- as.timeSeries(myPortfolioReturns)
#returns the portfolio with the minimal risk on the efficient frontier

minvarGARCH <- minvariancePortfolio(SimReturnsG) 
minew<-getPortfolio(minvarGARCH)
```

A fronteira eficiente está representada na figura \@ref(fig:frontgarch) sendo que o ponto a vermelho é onde se obtêm a mínima variância ou risco.

```{r,frontgarch,echo=FALSE, fig.cap='GARCH - Fronteira eficiente',fig.topcaption = TRUE,out.width = "80%"}
FronteiraEff <- portfolioFrontier(SimReturnsG, constraints = "LongOnly")
plot(FronteiraEff,c(1,2,3,4))
```

Neste modelo a empresa que apresenta maior peso continua a ser a ENEL com cerca de 61%, sendo que a Société Générale não é integrada no portfólio.

```{r,pgarchw,echo=FALSE}
minGARCH<- as.data.frame(minew$weights)
colnames(minGARCH)<- c("Pesos")
minGARCHk<- knitr::kable(
  minGARCH, caption = "GARCH - Pesos no portfolio",
  booktabs = TRUE)
kable_styling(minGARCHk , latex_options = "hold_position", position = "center")
```

Definidos as ponderações de cada empresa no portfolio podemos calcula o VaR assim como as contribuições individuais de cada um dos activos, podendo-se verificar nos dados abaixo que a maior contribuição continua a pertencer a empresa ENEL. O calculo do VaR foi efectuado considerando a aproximação Cornish-Fisher.

```{r}
garchweights <- c(0.2136498, 0.6196197, 0, 0.1667305)
ModVaR_95_GARCH <- VaR(myPortfolioReturns, p=0.95,weights=garchweights, portfolio_method = "component", method = "modified")
ModVaR_95_GARCH$contribution 
MVaRGARCH_VaR <- round(ModVaR_95_GARCH$MVaR,6)
```

Para um nível de confiança 95%, o VaR a 1 dia para dados simulados a 80 dias é de `r MVaRGARCH_VaR`, ou seja, existe uma probabilidade de 5% de ocorrer uma perda igual ou superior a este valor no montante total do portfolio investido.

## Volatilidade implicita

A volatilidade implícita é calculada de acordo com o modelo de Black-Scholes para opções europeias, sendo que o cálculo foi efectuado utilizando os valores presentes na tabela \@ref{tab:opcao}. Os valores apresentados referem-se ao valor de opções de compra, *call*^[https://www.eurexchange.com/exchange-en/products/equ/opt], para *strike price* próximos ou iguais aos valores das acções, ou seja, *at-the-money*. A data de vencimento das opções, ou maturidade, é a 18/09/2020.

No calculo da volatilidade implícita através da função R *EuropeanOptionImpliedVolatility()* utilizou-se a mesma taxa de juro sem risco que anteriormente, tendo que ser incluído o valor percentual referente ao dividendo pago no periodo de tempo em análise, sempre que as empresas o disponibilizaram aos acionistas.

```{r, opcao,echo=FALSE,fig.cap="Opções call sobre as empresas ",out.width="100%"}
knitr::include_graphics("image/opcao.png")
```
\FloatBarrier
\centering 
Fonte:Elaboração própria.

\justifying
\bigskip

\scriptsize
```{r}
#R CODE
TEFImp <- EuropeanOptionImpliedVolatility(type="call", value = 0.27, underlying = 4.40, strike = 4.40,
                                          dividendYield = 0.0407, riskFreeRate = -0.575, 
                                          maturity = 0.31746, volatility = 0.1)
ENELImp <- EuropeanOptionImpliedVolatility(type="call", value = 0.26, underlying = 7, strike = 7.2,
                                           dividendYield = 0.012, riskFreeRate = -0.575,
                                           maturity = 0.31746, volatility = 0.1)
SOGImp <- EuropeanOptionImpliedVolatility(type="call", value = 1.62, underlying = 13.82, strike = 14,
                                          dividendYield = 0, riskFreeRate = -0.575, maturity = 0.31746, 
                                          volatility = 0.1)
AIRImp <- EuropeanOptionImpliedVolatility(type="call", value = 6.91, underlying = 59.44, 
                                          strike = 60,dividendYield = 0, riskFreeRate = -0.575, 
                                          maturity = 0.31746, volatility = 0.1)
```
\normalsize

Na tabela \@ref(tab:tablevi) podemos ver os valores calculados para a volatilidade que permitem igualar os valores para as cotações das opções no dia 01/06/2020.

```{r, tablevi,echo=FALSE}
TEFImp <- round(TEFImp[]/sqrt(252),6)
ENELImp<-round(ENELImp[]/sqrt(252),6)
SOGImp<-round(SOGImp[]/sqrt(252),6)
AIRImp<-round(AIRImp[]/sqrt(252),6)

pimpvol<-cbind(TEFImp,ENELImp,SOGImp,AIRImp)
colnames(pimpvol)<-c("TEF.RET","ENEL.RET","GLE.RET","AIR.RET")
rownames(pimpvol)<-"Volatilidade implícita"

pimpvol <- as.data.frame(pimpvol)
pimpvolk<- knitr::kable(
  pimpvol, caption = "Volatilidade Implícita",
  booktabs = TRUE)
kable_styling(pimpvolk , latex_options = "hold_position", position = "center")
```
\FloatBarrier
\centering 
Fonte:Elaboração própria.

\justifying
\bigskip

### Simulação Volatilidade implicita

```{r,echo=FALSE, warning=FALSE}
TEF.preco_inicial <- TEF.MC$Close[[nrow(TEF.MC$Close)]]
TEF.MC.sigma <- pimpvol$TEF.RET*sqrt(252)
TEF.MC.mean <- (mu-(TEF.MC.sigma^2)/2)*(1/252)

set.seed(9)
monte_carlo_sim <- matrix(nrow=N, ncol = M)
for(j in 1:M){
  monte_carlo_sim[[1,j]] <- TEF.preco_inicial
  for(i in 2:N){
    monte_carlo_sim[[i,j]] <- monte_carlo_sim[[i-1,j]]*
      exp(TEF.MC.mean+TEF.MC.sigma *
            rnorm(1,0,1)*sqrt(1/252))
  }
}

TEF.MC_MC_sim <- as_tibble(cbind(dia,monte_carlo_sim))
w <- str_c('Sim',seq(1,M))
w <- c('Dia',w)
names(TEF.MC_MC_sim) <- w
TEF.MC_MC_sim <- gather(TEF.MC_MC_sim, key='Simulation', value = 'Preço',-(Dia))
```

```{r,echo=FALSE, warning=FALSE}
ENEL.preco_inicial <- ENEL.MI$Close[[nrow(ENEL.MI$Close)]]
ENEL.MI.sigma <- pimpvol$ENEL.RET*sqrt(252)
ENEL.MI.mean <- (mu-(ENEL.MI.sigma^2)/2)*(1/252)

set.seed(10)
ENEL_monte_carlo_sim <- matrix(nrow=N, ncol = M)
for(j in 1:M){
  ENEL_monte_carlo_sim[[1,j]] <- ENEL.preco_inicial
  for(i in 2:N){
    ENEL_monte_carlo_sim[[i,j]] <- ENEL_monte_carlo_sim[[i-1,j]]*exp(ENEL.MI.mean+ENEL.MI.sigma * rnorm(1,0,1)*sqrt(1/252))
  }
}
ENEL.MI_MC_sim <- as_tibble(cbind(dia,ENEL_monte_carlo_sim))
w1 <- str_c('Sim',seq(1,M))
w1 <- c('Dia',w1)
names(ENEL.MI_MC_sim) <- w1
ENEL.MI_MC_sim <- gather(ENEL.MI_MC_sim, key='Simulation', value = 'Preço',-(Dia))
```

```{r,echo=FALSE, warning=FALSE}
GLE.preco_inicial <- GLE.PA$Close[[nrow(GLE.PA$Close)]]
GLE.PA.sigma <- pimpvol$GLE.RET*sqrt(252)
GLE.PA.mean <- (mu-(GLE.PA.sigma^2)/2)*(1/252)

set.seed(11)
GLE_monte_carlo_sim <- matrix(nrow=N, ncol = M)
for(j in 1:M){
  GLE_monte_carlo_sim[[1,j]] <- GLE.preco_inicial
  for(i in 2:N){
    GLE_monte_carlo_sim[[i,j]] <- GLE_monte_carlo_sim[[i-1,j]]*exp(GLE.PA.mean+GLE.PA.sigma * rnorm(1,0,1)*sqrt(1/252))
  }
}
GLE.PA_MC_sim <- as_tibble(cbind(dia,GLE_monte_carlo_sim))
w2 <- str_c('Sim',seq(1,M))
w2 <- c('Dia',w2)
names(GLE.PA_MC_sim) <- w2
GLE.PA_MC_sim <- gather(GLE.PA_MC_sim, key='Simulation', value = 'Preço',-(Dia))
```

```{r,echo=FALSE, warning=FALSE}
AIR.preco_inicial <- AIR.PA$Close[[nrow(AIR.PA$Close)]]
AIR.PA.sigma <- pimpvol$AIR.RET*sqrt(252)
AIR.PA.mean <- (mu-(AIR.PA.sigma^2)/2)*(1/252)

set.seed(12)
AIR_monte_carlo_sim <- matrix(nrow=N, ncol = M)
for(j in 1:M){
  AIR_monte_carlo_sim[[1,j]] <- AIR.preco_inicial
  for(i in 2:N){
    AIR_monte_carlo_sim[[i,j]] <- AIR_monte_carlo_sim[[i-1,j]]*exp(AIR.PA.mean+AIR.PA.sigma * rnorm(1,0,1)*sqrt(1/252))
  }
}
AIR.PA_MC_sim <- as_tibble(cbind(dia,AIR_monte_carlo_sim))
w3 <- str_c('Sim',seq(1,M))
w3 <- c('Dia',w3)
names(AIR.PA_MC_sim) <- w3
AIR.PA_MC_sim <- gather(AIR.PA_MC_sim, key='Simulation', value = 'Preço',-(Dia))
```

Simulação na figura \@ref{fig:simvi} para os 80 dias de cotação, utilizando os valores referentes a volatilidade implícita.

```{r,simvi,echo=FALSE, fig.cap='Simulação Monte Carlo para 80 dias',fig.topcaption = TRUE,out.width = "45%"}

par(mfrow  = c(2,2))
ggplot(TEF.MC_MC_sim,aes(x = Dia, y = Preço, Group = Simulation)) + geom_line(alpha = 0.1,colour = 'darkgreen') + ggtitle(str_c("TEF: ", M, " Simulações de Monte Carlo ao longo de ", N," dias negociação"))
ggplot(ENEL.MI_MC_sim,aes(x = Dia, y = Preço, Group = Simulation)) + geom_line(alpha = 0.1,colour = 'dodgerblue3') + ggtitle(str_c("ENEL: ", M, " Simulações de Monte Carlo ao longo de ", N," dias negociação"))
ggplot(GLE.PA_MC_sim,aes(x = Dia, y = Preço, Group = Simulation)) + geom_line(alpha = 0.1,colour = "darkorange") + ggtitle(str_c("GLE: ", M, " Simulações de Monte Carlo ao longo de ", N," dias negociação"))
ggplot(AIR.PA_MC_sim,aes(x = Dia, y = Preço, Group = Simulation)) + geom_line(alpha = 0.1,colour="deeppink") + ggtitle(str_c("AIR: ", M, " Simulações de Monte Carlo ao longo de ", N," dias negociação"))
```


```{r}
M.TEF.MC <- apply(monte_carlo_sim, 1, mean)
M.ENEL.MI <- apply(ENEL_monte_carlo_sim, 1,mean )
M.GLE.PA <- apply(GLE_monte_carlo_sim, 1,mean )
M.AIR.PA <- apply(AIR_monte_carlo_sim, 1,mean )
```

Da mesma forma que nos outros modelos, calcula-se a média referente aos valores simulados para cada uma das empresas, estando os últimos 5 valores de cada uma representadas na tabela \@ref{tab:SimVIk}.
```{r,SimVIk,echo=FALSE}
SimVI <- as.data.frame(cbind(M.TEF.MC,M.ENEL.MI,M.GLE.PA,M.AIR.PA))
SimVIk<- knitr::kable(
  tail(SimVI), caption = "Vol. Implicita - Valores simulados",
  booktabs = TRUE)
kable_styling(SimVIk , latex_options = "hold_position", position = "center")
```
\FloatBarrier
\centering 
Fonte:Elaboração própria.

\justifying
\bigskip

### Portfolio volatilidade ímplicita

```{r,echo=FALSE}
SimDate<-read_xlsx ("data/date.xlsx")
SDate<- as.Date(SimDate$Date)
SimTEF <- as.numeric(SimVI$M.TEF.MC)
SimTEF.MC <-xts(SimTEF,SDate)
SimENEL <- as.numeric(SimVI$M.ENEL.MI)
SimENEL.MI <-xts(SimENEL,SDate)
SimGLE <- as.numeric(SimVI$M.GLE.PA)
SimGLE.PA <-xts(SimGLE,SDate)
SimAIR <- as.numeric(SimVI$M.AIR.PA)
SimAIR.PA <-xts(SimAIR,SDate)

myPortfolio <-na.omit(merge(SimTEF.MC,SimENEL.MI,SimGLE.PA,SimAIR.PA))

myPortfolioReturns <- ROC(myPortfolio,type="discrete")[-1,]
colnames(myPortfolioReturns) <- c("SimTEF","SimENEL","SimGLE","SimAIR")
```

```{r, echo=FALSE}
SimReturnsVI <- as.timeSeries(myPortfolioReturns)
#returns the portfolio with the minimal risk on the efficient frontier
minvarVI <- minvariancePortfolio(SimReturnsVI) 
minew<-getPortfolio(minvarVI)
```

A definição do portfólio de acordo com o método da mínima variância encontra-se definido na tabela \@ref{tab:minVIk}, sendo que a figura \@ref{fig:frontVI} representa a fronteira eficiente para esse mesmo modelo.

```{r,frontVI,echo=FALSE, fig.cap='Volatilidade Implícita - Fronteira eficiente',fig.topcaption = TRUE,out.width = "80%"}
FronteiraEff <- portfolioFrontier(SimReturnsVI, constraints = "LongOnly")
plot(FronteiraEff,c(1,2,3,4))
```
Como podemos ver neste modelo, a empresa ENEL é a que continua a ter mais peso no portfolio com cerca de 40%, não tendo no entanto tanto peso como nos outros modelos, sendo que a empresa telefónica representa cerca de 32%, sendo o restante repartido pelas outras 2 empresas.

```{r,minVIk,echo=FALSE}
minVI<- as.data.frame(minew$weights)
colnames(minVI)<- c("Pesos")
minVIk<- knitr::kable(
  minVI, caption = "Vol Implicita- Pesos no portfolio",
  booktabs = TRUE)
kable_styling(minVIk , latex_options = "hold_position", position = "center")
```


```{r,echo=FALSE}
viweights <- c(0.3278427, 0.4001305, 0.1278029, 0.1442238)
ModVaR_95_VI <- VaR(myPortfolioReturns, p=0.95,weights=viweights, portfolio_method = "component", method = "modified")
ModVaR_95_VI$contribution 
MVaRVI_VaR <- round(ModVaR_95_VI$MVaR,6)
```

 No cálculo do VaR foi utilizado o VaR paramétrico com aproximação Cornish-Fisher, pois é mais representativo da realidade da distribuição dos retornos dos activos financeiros, sendo que para um nível de confiança 95%, o VaR a 1 dia para dados simulados a 80 dias é de `r MVaRVI_VaR`, ou seja, existe uma probabilidade de 5% de ocorrer uma perda igual ou superior a este valor no montante total do portfolio investido.
