% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  12pt,
  a4paper,
  openany]{book}
\usepackage{lmodern}
\usepackage{setspace}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
  \setmainfont[]{Arial}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[left=3.5cm,right=2cm,top=3cm,bottom=3cm, asymmetric]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\pagestyle{plain}
\usepackage{booktabs}
\usepackage{amsmath,amsfonts,amsthm,bm} % Math packages
\usepackage{mathtools, cases}
\usepackage{fontspec}
\usepackage[utf8]{inputenc}
\usepackage[portuguese]{babel}

\usepackage{etoolbox}

\usepackage{ragged2e}

\usepackage{titlesec}

\usepackage{setspace}

\usepackage{changepage}

\usepackage{graphicx}
\usepackage{placeins}

\usepackage{makeidx}
\makeindex
\usepackage[nottoc]{tocbibind}

\makeatletter
\patchcmd{\ttl@save@mkschap}{*}{}{}{}
\makeatother
\usepackage{indentfirst}
\usepackage{floatrow}
\floatsetup[figure]{capposition=top}
\floatsetup[table]{capposition=top}
\usepackage{subfig}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{apalike}

\author{}
\date{\vspace{-2.5em}}

\begin{document}

%titlepage
\thispagestyle{empty}
\begin{center}
\begin{minipage}{0.90\linewidth}
    \centering
%University logo
    \includegraphics[width=1\textwidth]{image/biglogo.png}

%Thesis title
    {\uppercase{\Large Impacto da volatilidade na otimiza√ß√£o de portfolios financeiros\par}}
    \vspace{2cm}
%Author's name
    {\Large Leonel da Silva Baptista\par}
    \vspace{2cm}
%Degree
    {\Large Mestrado em Estat√≠stica, Matem√°tica e Computa√ß√£o\par}
    {\Large Ramo Estat√≠stica Computacional\par}
    \vspace{2cm}

%Date
    {\Large 2020}
\end{minipage}
\end{center}


%titlepage
\thispagestyle{empty}
\begin{center}
\begin{minipage}{0.90\linewidth}
    \centering
%University logo
    \includegraphics[width=1\textwidth]{image/logo.png}

%Thesis title
    {\uppercase{\Large Impacto da volatilidade na otimiza√ß√£o de portfolios financeiros\par}}
    \vspace{2cm}
%Author's name
    {\Large Leonel da Silva Baptista\par}
    \vspace{2cm}
%Degree
    {\Large Mestrado em Estat√≠stica, Matem√°tica e Computa√ß√£o\par}
    {\Large Ramo Estat√≠stica Computacional\par}
    \vspace{2cm}
%oriented
    {\Large Disserta√ß√£o orientada pelo\par}
    {\Large Professor Doutor Am√≠lcar Manuel do Ros√°rio Oliveira\par}
    \vspace{2cm}
%Date
    {\Large 2020}
\end{minipage}
\end{center}

\pagenumbering{gobble}% Remove page numbers (and reset to 1)

\clearpage

\chapter*{Resumo}
\fontsize{12}{21}\selectfont
A presente disserta√ß√£o t√™m como √¢mbito a an√°lise de tr√™s m√©todos diferentes de obter a volatilidade de instrumentos financeiros, nomeadamente valores mobili√°rios, e seu consequente impacto no resultado da rentabilidade de portfolios constitu√≠dos utilizando como pressuposto a m√©dia vari√¢ncia, assim como a sua exposi√ß√£o ao risco, sendo que a volatilidade constitui uma pe√ßa central na constitui√ß√£o de determinados instrumentos financeiros e respetivo calculo de exposi√ß√£o ao risco.

Os m√©todos analisados para calculo da volatilidade s√£o a M√©dia Movel Exponencial (EWMA), o modelo da heteroscedasticidade condicional auto-regressiva generalizada (GARCH) e a volatilidade impl√≠cita. Os dois primeiros m√©todos t√™m como subjacentes dados hist√≥ricos dos instrumentos financeiros, sendo que a volatilidade impl√≠cita √© a volatilidade esperada pelo mercado, sendo obtida atrav√©s da cota√ß√£o das op√ß√µes dos respetivos subjacentes.

A an√°lise de risco √© efetuada aplicando dois m√©todos complementares de an√°lise. O Value at Risk (VaR), que contempla a percentagem de perdas que excedem o VaR e o Expected shortfall (ES), que contempla a magnitude dessas perdas.

Este trabalho √© realizado tendo como ferramenta de apoio a linguagem de programa√ß√£o R.
\bigbreak

\noindent\textbf{Palavras chave:} Volatilidade, portfolio, rentabilidade, risco, R




\pagenumbering{roman}% Arabic page numbers (and reset to 1)
\setcounter{page}{2}

\chapter*{Abstract}
\fontsize{12}{21}\selectfont
The scope of this dissertation is to analyze three different methods of obtaining the volatility of financial instruments, namely securities, and their consequent impact on the profitability of portfolios constituted using the assumption of mean-variance, as well as their exposure to risk, volatility is a central element in the constitution of certain financial instruments and the respective calculation of exposure to risk.

The methods analyzed for calculating the volatility are the Exponential Moving Average (EWMA), the generalized autoregressive conditional heteroscedasticity model (GARCH) and the implied volatility. The first two methods are based on historical data on financial instruments, the implied volatility being the volatility expected by the market, being obtained through the quotation of the options of the respective underlying.

The risk analysis is carried out using two complementary methods of analysis. Value at Risk (VaR), which includes the percentage of losses that exceed VaR and Expected shortfall (ES), which considers the magnitude of these losses.

This work is carried out using the R programming language as a support tool.

\bigbreak

\noindent\textbf{Keywords:} Volatility, portfolio, profitability, risk, R

\newenvironment{dedication}
  {\clearpage           % we want a new page
   \itshape             % the text is in italics
   \raggedleft          % flush to the right margin
  }
  {\par % end the paragraph
   \vspace{\stretch{3}} % space at bottom is three times that at the top
   \clearpage           % finish off the page
  }
\begin{dedication}
{\Large Dedicado a minha esposa\par}
\end{dedication}

\chapter*{Agradecimentos}

\renewcommand*\contentsname{√çndice}
{
\setcounter{tocdepth}{2}
\tableofcontents
}
\listoftables
\listoffigures
\setstretch{1.5}
\hypertarget{simbologia-e-notauxe7uxf5es}{%
\chapter*{Simbologia e nota√ß√µes}\label{simbologia-e-notauxe7uxf5es}}
\addcontentsline{toc}{chapter}{Simbologia e nota√ß√µes}

\mainmatter

\hypertarget{intro}{%
\chapter*{Introdu√ß√£o}\label{intro}}
\addcontentsline{toc}{chapter}{Introdu√ß√£o}

A estat√≠stica aplicada ao sector financeiro t√™m sido pratica comum nas √∫ltimas d√©cadas, sendo que a sua aplica√ß√£o n√£o se resume apenas a estat√≠stica descritiva, tendo vindo a beneficiar dos avan√ßos verificados na aplica√ß√£o de ferramentas estat√≠sticas para analise preditiva dos dados, devido essencialmente aos avan√ßos tecnol√≥gicos no hardware de equipamentos inform√°ticos que permitem a aplica√ß√£o de algoritmos mais complexos que de outro modo n√£o seria poss√≠vel utilizar, pelo menos em tempo √∫til.

Um dos sectores com grande aplicabilidade dos m√©todos matem√°ticos e estat√≠sticos nas finan√ßas √© a analise quantitativa, sendo as principais √°reas de aplica√ß√£o a estrutura√ß√£o de derivados, gest√£o do risco, trading autom√°tico e gest√£o de investimentos.

Historicamente, a analise quantitativa iniciou-se em 1900 com Louis Jean-Baptiste Alphonse Bachelier, onde a sua tese de doutoramento forneceu um modelo para estipular o pre√ßo de op√ß√µes considerando uma distribui√ß√£o normal (\url{https://en.wikipedia.org/wiki/Quantitative_analysis_(finance)}).

Na d√©cada de 50, Harry Markowitz escreveu um artigo intitulado ``Portfolio Selection'' que viria a revolucionar o modo como selecionar uma carteira de instrumentos financeiros, aplicando princ√≠pios de correla√ß√£o e vari√¢ncia de modo a constituir portfolios de a√ß√µes , onde a ``fronteira eficiente'' representa portfolios que maximizam retornos de acordo com o risco assumido, providenciando modelos que demonstravam que s√≥ com a diversifica√ß√£o de investimentos √© que se conseguiria atingir a efici√™ncia, embora s√≥ bastante mais tarde esta teoria come√ßa-se a ver a sua aplicabilidade nas institui√ß√µes financeiras. A aplicabilidade de t√©cnicas estat√≠sticas fica saliente quando Markowitz estipula que ``para usar a regra da m√©dia-vari√¢ncia na sele√ß√£o de a√ß√µes devemos ter procedimentos para encontrar \(\mu_i\) e \(\sigma_{ij}\) razo√°veis. Estes procedimentos, eu acredito, devem combinar t√©cnicas estat√≠sticas e julgamento pr√°tico do Homem'' \citep[pp.91]{Markowitz1952}. As limita√ß√µes do modelo prendem-se com pressuposto que n√£o representam exatamente a realidade, como o pressuposto de que os retornos das a√ß√µes seguem uma distribui√ß√£o normal, sendo que a distribui√ß√£o dos retornos segue muitas vezes uma distribui√ß√£o com curtose leptoc√∫rtica, apresentando caudas pesadas e um pico superior ao da distribui√ß√£o normal.

Na d√©cada de 60, Sharpe, Lintner and Mossin desenvolveram um modelo para equil√≠brio de mercado, definido como \emph{Capital Asset Pricing Model} (CAPM), descrevendo a rela√ß√£o entre risco sistem√°tico e o retorno esperado. O modelo pressup√µe que, se todos os investidores cont√™m o mesmo portfolio, ent√£o, em equil√≠brio esse deve ser o portfolio de mercado. De acordo com \citet{Sharpe1964}, em equil√≠brio os pre√ßos dos ativos de capital foram ajustados de forma que o investidor consiga atingir qualquer ponto desejado ao longo da reta do mercado capital, ou \emph{capital market line} (CMP), pressupondo que o investidor siga uma estrat√©gia de diversifica√ß√£o do investimento. O CMP pode ser utilizado de modo a otimizar um portfolio caso seja contemplado uma taxa de juro sem risco na sua estrutura√ß√£o, sendo o ponto tangente a curva denominada de fronteira eficiente.

Tamb√©m na d√©cada de 60 foi apresentado pela primeira vez o modelo de Black-Scholes-Merton, fornecendo uma solu√ß√£o para valorizar op√ß√µes europeias e outros derivados. O modelo assume que os pre√ßos t√™m uma distribui√ß√£o lognormal e que a volatilidade √© constante ao longo do tempo. A volatilidade que √© assumida neste modelo √© a volatilidade impl√≠cita da op√ß√£o, ou seja, a volatilidade para o qual o valor dado pelo modelo Black-Scholes-Merton iguala o pre√ßo de mercado. Outros modelos foram desenvolvidos de modo a valorizar op√ß√µes e outros derivados financeiros, entre eles a arvore binomial e simula√ß√£o de Monte Carlo.

Os v√°rios modelos apresentados t√™m como finalidade contribuir para uma decis√£o mais informada por parte do investidor, sendo que na generalidade o investidor ir√° optar pelo investimento que apresenta um maior retorno de acordo com o risco a que se disp√µem estar exposto, tendo em considera√ß√£o o seu perfil e expectativas.A escolha do portf√≥lio √© feita resolvendo um problema de otimiza√ß√£o, no qual o risco do portf√≥lio √© minimizado sendo definido como retri√ß√£o o valor desejado de retorno esperado. Desta forma √© importante quantificar o risco. Existem v√°rios modelos para quantificar o risco, ou seja quantificar a perda esperada de acordo com a hip√≥tese de ocorr√™ncia de determinado cen√°rio, sendo que 2 desses modelos para an√°lise do risco s√£o o \emph{Value at Risk} (VaR) e o \emph{Expected Shortfall}, tamb√©m denominado \emph{Conditional Value at Risk} (CVaR). De acordo com \citet{HistVaR}, em 1922 no New York Stock Exchange j√° eram exigidos requisitos de capital a alguns dos seus membros, tendo na d√©cada de 50 Markowitz e Roy, separadamente, publicado metodos para quantificar o VaR, sendo ambos bastante similares e com a finalidade de quantificar o risco a que estaria exposto um portfolio. A necessidade de utilizar medidas de risco mais sofisticada tornou-se mais vis√≠vel na d√©cada de 80 devido ao aparecimento de produtos mais complexos e ao aumento da volatilidade dos mercados, sendo que devido a regulamenta√ß√£o cada vez mais exigente, como o Basel III e Solvency II, as institui√ß√µes financeiras e seguradoras devem implementar mecanismos de gest√£o de risco, sendo que os requisitos de capital s√£o bastante mais exigentes desde a ocorr√™ncia da crise do subprime em 2007. De acordo com os acordos de Basel o VaR deve ser estimado diariamente utilizando o percentil 99\textsuperscript{th}.

Todos estes modelos t√™m tido aplicabilidade na an√°lise quantitativa financeira, sendo que novos modelos foram sendo desenvolvidos ou apenas melhorados de modo a dar resposta √† realidade verificada no mercado. Como podemos subentender uma das disciplinas fundamentais na defini√ß√£o destes modelos √© o conhecimento de estat√≠stica e a sua aplica√ß√£o pr√°tica, quer atrav√©s de t√©cnicas param√©tricas, onde se assume um pressuposto forte de que os valores de uma vari√°vel t√™m uma distribui√ß√£o normal, seja atrav√©s de t√©cnicas n√£o param√©tricas, onde n√£o se assume que a distribui√ß√£o dos valores de uma vari√°vel apresentam distribui√ß√£o normal.

O trabalho desenvolvido ao longo desta tese, prop√µem-se analisar o impacto que poder√° ter o m√©todo de calculo da volatilidade sobre a defini√ß√£o de um portfolio, o seu retorno esperado assim como quantificar o risco a que se estar√° exposto. Desde os primeiros trabalhos de \citet{Markowitz1952} acerca da otimiza√ß√£o de portfolios que v√°rios outros trabalhos foram desenvolvidos para constitui√ß√£o e otimiza√ß√£o de portfolios, sendo que nesta disserta√ß√£o se ir√° aplicar o m√©todo introduzido por \citet{Markowitz1952}, utilizando a teoria da m√©dia-vari√¢ncia para a constitui√ß√£o de um portfolio. O mesmo se poder√° afirmar acerca do c√°lculo da exposi√ß√£o ao risco por um portfolio, sendo, no entanto, o VaR e o CVaR dois dos m√©todos mais utilizados para quantificar essa m√©trica, sendo que de acordo com \citet{OptVaR2000} o CVaR √© conhecido por ter melhores propriedades que o VaR. Como iremos ver existem diferentes m√©todos de calculo do VaR e CVaR, sendo que neste trabalho iremos utilizar a que for mais adequada ao tipo de dados em an√°lise.

Como inicialmente referido, o objetivo √© analisar as diferentes formas de calcular a vari√¢ncia, sendo que iremos focar-nos em tr√™s formas diferentes de calcular esse valor, an√°lisar a sua implica√ß√£o no resultado final, assim como o valor que resulta da quantifica√ß√£o de risco e o seu impacto na perce√ß√£o pelo investidor.

A investiga√ß√£o decorrer√° aplicando-se os diferentes m√©todos de calculo da vari√¢ncia a dados do mercado, assim como os pressupostos te√≥ricos de cada um dos m√©todos, sendo constitu√≠da uma carteira de a√ß√µes integrantes do Euro Stoxx 50, calculando o seu retorno final para cada uma das volatilidades, assim como o risco a que est√° exposto um investidor. O retorno obtido ser√° tamb√©m comparado com um \emph{benchmark}, neste caso o Euro Stoxx 50.

Ao longo deste percurso ser√° tamb√©m an√°lisada a forma como √© constitu√≠dos um portfolio, a teoria subjacente a m√©dia-vari√¢ncia, as retri√ß√µes aplic√°veis ao modelo desenvolvido, assim como a teoria subjacente aos v√°rios m√©todos para an√°lise da exposi√ß√£o ao risco aquando da constitui√ß√£o de um portfolio.

A aplica√ß√£o pratica dos m√©todos aos dados do mercado ser√° realizado com o apoio da linguagem de programa√ß√£o R, utilizando para esse efeito os v√°rios ``packages'' dispon√≠veis para aplica√ß√£o ao sector financeiro. Um dos cap√≠tulos ser√° dedicado a apresenta√ß√£o e descri√ß√£o dos principais ``packages'' utilizados para an√°lise dos dados, sendo que o R √© parte integrante desta disserta√ß√£o como ferramenta de an√°lise estat√≠sticas e aplicabilidade ao sector financeiro.

\begingroup
\titleformat{\chapter}[display]
{\normalfont\huge\bfseries\centering}{\chaptertitlename\ \thechapter}{20pt}{\Huge}

\hypertarget{modelauxe7uxe3o-estatuxedstica-na-otimizauxe7uxe3o-de-portfuxf3lios}{%
\chapter{Modela√ß√£o Estat√≠stica na Otimiza√ß√£o de Portf√≥lios}\label{modelauxe7uxe3o-estatuxedstica-na-otimizauxe7uxe3o-de-portfuxf3lios}}

\newpage

\hypertarget{series-temporais}{%
\section{Series Temporais}\label{series-temporais}}

O valor da cota√ß√£o de activos financeiros, como ac√ß√µes ou op√ß√µes, s√£o representados de forma sequencial ao longo do tempo, considerando determinado intervalo, que pode ser segundos, minutos, dias, semanas ou outro intervalo considerado √∫til para representa√ß√£o dos dados ao longo do tempo. Quando tal acontece estamos em presen√ßa de series temporais.

Os pre√ßos de activos financeiros ao longo do tempo formam o que √© denominado por processos estoc√°sticos. Processos estoc√°sticos s√£o uma classe de series temporais onde o valor da vari√°vel muda ao longo de tempo de forma aleat√≥ria. Os processo estoc√°sticos podem ser classificados de discretos ou cont√≠nuos, sendo que na an√°lise de activos financeiros, embora estes sigam processos discretos, ser√£o considerados processos estoc√°sticos cont√≠nuos ao longo do tempo, sendo que estes modelos acabam por ser bastante √∫teis na modeliza√ß√£o dos pre√ßos de activos financeiros de acordo com \citet{Hull2018}.

\hypertarget{modelauxe7uxe3o-do-preuxe7o-de-acuxe7uxf5es}{%
\subsection{Modela√ß√£o do pre√ßo de ac√ß√µes}\label{modelauxe7uxe3o-do-preuxe7o-de-acuxe7uxf5es}}

Os pre√ßos das ac√ß√µes seguem habitualmente o que √© conhecido por um processo de Markov, onde apenas o valor presente importa para prever o valor futuro. Desta forma a √∫nica informa√ß√£o relevante √© o seu valor no momento, sendo que os valores e trajecto verificado no passado n√£o ir√£o ter import√¢ncia na defini√ß√£o da distribui√ß√£o probabil√≠stica do pre√ßo no futuro \citep{Hull2018}.Desta forma as propriedades de Markov no pre√ßo das ac√ß√µes √© consistente com a efici√™ncia dos mercados na forma fraca, constituindo esta uma das tr√™s formas de efici√™ncia de mercado definidas por Eugene Fama\footnote{Ver ``\url{https://pt.wikipedia.org/wiki/Eugene_Fama}''}.

O modelo representado aqui e que ser√° utilizado para simular pre√ßos de activos financeiros atravessou v√°rios pressupostos at√© a sua conclus√£o final, sendo apresentado de uma forma muito sucinta as principais etapas de desenvolvimento desse modelo.

Um dos primeiros modelos era representado por um processo de Wiener, sendo este um processo particular do processo estoc√°stico Markov, referido como movimento Browniano, descrevendo a evolu√ß√£o de uma vari√°vel com distribui√ß√£o normal padr√£o. A modela√ß√£o pressup√µe que a vari√°vel z possa ser dada pela seguinte equa√ß√£o:

\begin{equation} 
  dz = \varepsilon \sqrt{dt}\qquad   \varepsilon \sim N(0,1);
  \label{eq:wiener}
\end{equation}

Desta forma a vari√°vel z segue um processo de Wiener, sendo uma vari√°vel independente e identicamente distribu√≠da (i.i.d).

Como este modelo n√£o preenchia todos os pressupostos, aplicou-se um processo generalizado de Wiener (tamb√©m conhecido como movimento Browniano (BM)), onde √© incluindo um amplificador/redutor na parte aleat√≥ria do processo de forma a ajustar o processo a propriedades especificas de cada ac√ß√£o, ilustrado na equa√ß√£o \eqref{eq:gwiener}. Este processo descreve a evolu√ß√£o de um processo de uma vari√°vel com distribui√ß√£o normal, com um desvio \(\mu\) por unidade de tempo e uma taxa de vari√¢ncia de \(\sigma^2\) tamb√©m por unidade de tempo.

\begin{equation} 
  \delta S = \mu\delta t +\sigma\varepsilon\sqrt{\delta t}\qquad\varepsilon \sim N(0,1);
  \label{eq:gwiener}
\end{equation}

A diferen√ßa relativo a um processo de Wiener √© que no processo generalizado de Wiener a taxa de desvio e vari√¢ncia pode ter como valor qualquer constante, sendo que no processo de Wiener esses valores s√£o de 0 e 1 respectivamente.

Outro processo estoc√°stico, conhecido como processo It√¥, foi desenvolvido, representando um processo generalizado de Wiener, sendo que neste caso o valor dos par√¢metros \(\mu\) e \(\sigma\) s√£o fun√ß√µes do valor subjacente da vari√°vel S e do tempo t. Este modelo √© o que vai ser utilizado para simula√ß√£o do pre√ßo das ac√ß√µes sendo representado pelas seguintes equa√ß√µes:

\begin{equation} 
  \frac{\delta S}{S} = \mu\delta t +\sigma\varepsilon\sqrt{\delta t}\qquad\varepsilon \sim N(0,1);
  \label{eq:ito}
\end{equation}
\begin{equation} 
  In S_1 - InS_0 \approx\phi\Big[\Big(\mu-\frac{\sigma^2}{2}\Big)\delta t, \sigma\sqrt{\delta t}\Big]
  \label{eq:Inito}
\end{equation}
\begin{equation} 
  S_1 =S_0 e^{\Big(\mu-\frac{\sigma^2}{2}\Big)\delta t + \sigma\varepsilon\sqrt{\delta t}}
  \label{eq:logprice}
\end{equation}

Nesta equa√ß√£o o \(\mu\) representa a taxa de retorno anual esperado, sendo que \(\sigma\) representa o desvio padr√£o ou volatilidade da ac√ß√£o, par√¢metro muito importante para a determina√ß√£o do valor de v√°rios derivados, havendo v√°rias formas de calcular este valor como veremos mais adiante. Estes valores s√£o fun√ß√£o do corrente valor de S e do tempo actual t. Numa economia neutra face ao risco \(\mu\) √© igual a taxa de juro sem risco. O \(\varepsilon\) representa uma vari√°vel com distribui√ß√£o normal(N(0,1)).

Este processo √© conhecido como movimento Browniano geom√©trico (GBM). De acordo com \citet{AppliedFinancial} um ``GBM pode ser considerado como um movimento probabil√≠stico cont√≠nuo no qual o logaritmo da quantidade que varia aleatoriamente segue um movimento Browniano (processo de Wiener) com desvio.'' Este modelo √© a base do modelo de Black-Scholes, que ser√° revisto mais adiante, aquando do c√°lculo da volatilidade impl√≠cita nas op√ß√µes.

Tendo em considera√ß√£o a equa√ß√£o \eqref{eq:logprice}, \emph{S} segue uma distribui√ß√£o lognormal. Uma distribui√ß√£o lognormal √© mais real√≠stico de acordo com o movimento do pre√ßo das ac√ß√µes, prevenindo que o valor se torne negativo.

Em simula√ß√£o, este processo √© realizado considerando uma distribui√ß√£o normal com m√©dia e vari√¢ncia dada pela equa√ß√£o \eqref{eq:Inito} \citep{FRM1}.

\hypertarget{simulauxe7uxe3o-de-monte-carlo}{%
\subsection{Simula√ß√£o de Monte Carlo}\label{simulauxe7uxe3o-de-monte-carlo}}

A simula√ß√£o de Monte Carlo consiste na gera√ß√£o de valores a partir de uma determinada distribui√ß√£o ou amostra, sendo que ``o termo Monte Carlo √© usado para se referir a t√©cnicas que envolvem simula√ß√£o computacional''\citep[pp.457]{ProgSim}.

Ao simular os pre√ßos dos activos financeiros iremos utilizar a t√©cnica de Monte Carlo de modo a gerar amostras aleat√≥rias de acordo com a equa√ß√£o \eqref{eq:logprice}. Os pre√ßos ser√£o simulados criando v√°rias tentativas com valores aleat√≥rios para \(\varepsilon\) a partir de \(\phi\)(0,1). A precis√£o dos valores obtidos na simula√ß√£o depende do n√∫mero de tentativas efectuadas, sendo considerado 1000 tentativas por cada dia e considerar o valor m√©dio dessas tentativas como o valor simulado para cada um dos dias simulados.

\hypertarget{volatilidade}{%
\section{Volatilidade}\label{volatilidade}}

Na ind√∫stria financeira o desvio padr√£o associado a instrumentos financeiros √© definido como volatilidade. Desta forma quando estamos a falar de volatilidade estamos a considerar o desvio padr√£o observado.

De acordo com \citet{HullRisk2018}:

\begin{spacing}{1}
\begin{adjustwidth}{28.3464pt}{28.3464pt}\footnotesize
"A vari√°vel volatilidade, $\sigma$, √© definida como o desvio padr√£o do retorno fornecido pela vari√°vel por unidade de tempo quando o retorno √© expresso usando juros compostos. Quando a volatilidade √© usada para valorizar op√ß√µes, a unidade de tempo √© normalmente 1 ano, sendo a volatilidade o desvio padr√£o do retorno compostos por ano. Quando a volatilidade √© usada para a gest√£o de risco, a unidade de tempo usual √© 1 dia de forma a que a volatilidade √© o desvio padr√£o do retorno composto por dia." (pp.213-214)
\normalsize\end{adjustwidth}
\end{spacing}
\medskip

V√°rios s√£o os modelos para calculo da volatilidade, sendo que um dos modelos mais simples √© o do c√°lculo da vari√¢ncia de acordo com a equa√ß√£o \eqref{eq:estdesviopadrao} utilizando os dados hist√≥ricos dos retornos, sendo que neste modelo o peso atribu√≠do aos dados √© o mesmo independentemente da sua antiguidade. Outros modelos como a M√©dia M√≥vel Exponencial Ponderada (EWMA) e Heteroscedasticidade condicional auto-regressiva generalizada (GARCH) s√£o modelos, que embora utilizem dados hist√≥ricos, reconhecem que tanto a volatilidade como a correla√ß√£o n√£o s√£o constantes ao longo do tempo, atribuindo maior peso aos dados mais recentes. Por fim, um dos modelos que n√£o integra dados hist√≥ricos mas sim as expectativas futuras para o c√°lculo da volatilidade √© o da volatilidade impl√≠cita, utilizando o modelo de Black-Scholes-Merton na valoriza√ß√£o das respectivas op√ß√µes relativamente ao subjacente.

\hypertarget{autoregressive-conditional-heteroscedasticityarch}{%
\subsection{AutoRegressive Conditional Heteroscedasticity(ARCH)}\label{autoregressive-conditional-heteroscedasticityarch}}

Existem equa√ß√µes para o c√°lculo da vari√¢ncia que atribuem igual peso aos valores passados independentemente da sua antiguidade. Outros m√©todos atribuem mais peso aos dados mais recentes, pois pressup√µem-se que esses carregam mais informa√ß√£o √∫til acerca da vari√°vel em estudo, criando modelos que prev√™em com menor erro os valores futuros em estudo.

Uma das formas de c√°lculo da volatilidade tendo em considera√ß√£o os valores mais recentes dos retornos √© aplicando a equa√ß√£o

\begin{equation} 
  \sigma_{n}^{2} =\sum_{i=1}^{m}\alpha_i\mu_{n-1}^{2}
  \label{eq:weight}
\end{equation}

A vari√°vel \(\alpha\) representa o peso associado a observa√ß√£o \emph{i} dos retornos, sendo que a soma dos respectivos pesos deve ser igual a 1.

Tendo em considera√ß√£o \citet{Hull2018} se a m√©dia a longo prazo da taxa de vari√¢ncia for considerada e assignado um peso a ela ent√£o teremos

\begin{equation} 
  \sigma_{n}^{2} =\gamma\mathcal{V}_{L}+\sum_{i=1}^{m}\alpha_i\mu_{n-1}^{2}
  \label{eq:arch}
\end{equation}

sendo que \(\mathcal{V}_{L}\) √© a taxa de vari√¢ncia de longo prazo e \(\gamma\) √© o peso associado a cada \(\mathcal{V}_{L}\), sendo que os repectivos pesos devem somar 1, de acordo com \[\gamma + \sum_{i=1}^{m}\alpha_i = 1\]

A equa√ß√£o \eqref{eq:arch} √© conhecida como um modelo \emph{AutoRegressive Conditional Heteroscedasticity} ou ARCH(m)\footnote{Introduzido por Robert F.Engle em 1982, tendo-lhe sido atribu√≠do o pr√©mio Nobel da economia em 2003}, sendo que em finan√ßas √© uma das classes de modelos mais utilizada para prever a volatilidade. O \emph{m} representa o n√∫mero de observa√ß√µes, sendo que a vari√°vel aleat√≥ria \(\mu\) segue uma distribui√ß√£o normal, sendo no entanto pr√°tica comum considerar outros tipos de distribui√ß√µes como a distribui√ß√£o t-student, devido ao factos da distribui√ß√£o dos retornos apresentarem caudas mais pesadas que uma distribui√ß√£o normal.

Auto-regressivo significa que a vari√¢ncia presente depende do seu pr√≥prio passado e n√£o num regressor ex√≥geno. O facto de ser condicional significa que a vari√¢ncia de amanh√£ depende das vari√¢ncias mais recentes. J√° a heteroscedasticidade significa que a vari√¢ncia n√£o √© constante ao longo do tempo. Quando a vari√¢ncia e a covari√¢ncia variam ao longo do tempo falamos de heteroscedasticidade condicional.

O pressuposto destes modelos √© de que a vari√¢ncia dos retornos seguem um processo previs√≠vel, sendo que os modelos ARCH s√£o menos convincentes na previs√£o quando a volatilidade t√™m altera√ß√µes bruscas. A apresenta√ß√£o do modelo ARCH serve como forma introdut√≥ria de apresenta√ß√£o aos modelos \emph{exponentially weighted moving average} (EWMA) e \emph{generalized autoregressive conditional heteroskedasticity} (GARCH).

\hypertarget{exponentially-weighted-moving-averageewma}{%
\subsection{Exponentially weighted moving average(EWMA)}\label{exponentially-weighted-moving-averageewma}}

O modelo da m√©dia m√≥vel exponencial ponderada (EWMA) √© um estimador calculado a partir de amostras sequenciais da popula√ß√£o onde √© atribuindo maior peso aos valores mais recentes, sendo que esse peso diminui exponencialmente a medida que vamos recuando no tempo. A formula utilizada para actualizar o valor para a volatilidade √©

\begin{equation} 
  \sigma_{n}^{2} = \lambda\sigma_{n-1}^{2}+(1-\lambda)\mu_{n-1}^{2} 
  \label{eq:ewma}
\end{equation}
sendo um caso particular da equa√ß√£o \eqref{eq:weight}. Neste caso \(\gamma_{i+1} = \lambda\gamma_{i}\), onde 0\textless{}\(\lambda\leq1\) √© uma constante, sendo \(\sigma_{n-1}\) a estimativa da volatilidade realizada no dia n-2 para o dia n-1, e \(\mu_{n-1}\) a mudan√ßa percentual di√°ria mais recente. Ao valor de \(\lambda\) atribuido ``estudos emp√≠ricos mostram que o \emph{decay factor} (\(\lambda\)) 0,94 fornece uma boa estimativa para todos os ativos.''\citep[p.4]{riskmetrics}

De modo a entender porque √† volatilidade obtida atrav√©s da EWMA correspondem pesos que diminuem exponencialmente vamos substituir \(\sigma_{n-1}^{2}\) na equa√ß√£o \eqref{eq:ewma} de modo a obter

\begin{equation} 
  \sigma_{n}^{2} = (1-\lambda)(\mu_{n-1}^{2}+\lambda\mu_{n-2}^{2})+\lambda^{2}\sigma_{n-2}^{2}
  \label{eq:ewma1}
\end{equation}

Continuando a substituir acabamos por obter

\begin{equation} 
  \sigma_{n}^{2} = (1-\lambda)\sum_{i=1}^{m}\lambda^{i-1}\mu_{n-1}^{2}+\lambda^{m}\mu_{n-m}^{2}
  \label{eq:ewma2}
\end{equation}

Como podemos ver pela equa√ß√£o \eqref{eq:ewma2} o ``peso de \(\mu_{i}\) decresce a uma taxa \(\lambda\) a medida que vamos recuando no tempo. Cada peso √© \(\lambda\) vezes o peso anterior.'' \citep[p.226]{HullRisk2018}. Como os pesos diminuem geometricamente, o EWMA tamb√©m √© referido como m√©dia m√≥vel geom√©trica.

\hypertarget{generalized-autoregressive-conditional-heteroskedasticitygarch}{%
\subsection{Generalized autoregressive conditional heteroskedasticity(GARCH)}\label{generalized-autoregressive-conditional-heteroskedasticitygarch}}

Em 1986 um modelo mais flex√≠vel que o modelo ARCH foi proposto, sendo este o modelo \emph{Generalized} ARCH\footnote{Introduzido por Bollerslev(1986)}. De acordo com \citet{volatilitymodels} ``Uma grande vantagem dos modelos GARCH √© que os retornos n√£o s√£o assumidos independentes, e mesmo se forem assumidos condicionais gaussianos para retornos anteriores, incondicionalmente, eles n√£o s√£o gaussianos, porque o agrupamento de volatilidade gera leptocurtose''(p.2). A equa√ß√£o que define o modelo GARCH(1,1) √©

\begin{equation} 
  \sigma_{n}^{2} = \gamma\mathcal{V}_{L} + \alpha\mu_{n-1}^{2} + \beta\sigma_{n-1}^{2}
  \label{eq:garch}
\end{equation}

Nesta equa√ß√£o, verifica-se que \(\sigma_{n}^{2}\) √© calculado a partir da taxa de varia√ß√£o m√©dia de longo prazo \(\mathcal{V}_{L}\), como tamb√©m a partir de \(\sigma_{n-1}\) e \(\mu_{n-1}\), sendo que o modelo EWMA √© um caso particular do modelo GARCH(1,1) onde \(\gamma=0\), \(\alpha=1-\lambda\) e \(\beta=\lambda\). Tal como referido nos modelos anteriores, a soma dos diferentes pesos deve ser igual a 1, sendo \[\gamma+\sigma+\beta=1\]
O modelo gen√©rico de GARCH √© GARCH(p,q) onde o modelo calcula \(\sigma_{n}^{2}\) a partir das observa√ß√µes \emph{p} mais recentes de \(\mu^2\) e as mais recentes estima√ß√µes \emph{q} da taxa de vari√¢ncia. Desta forma, o modelo "``(1,1) em GARCH(1,1) indica que \(\sigma_{n}^{2}\) √© baseado nas mais recentes observa√ß√µes de \(\mu^2\) e nas mais recentes estima√ß√µes da taxa de vari√¢ncia.''\citep[p.227]{Hull2018}

Considerando \(\omega=\gamma\mathcal{V}_{L}\), o modelo GARCH(1,1) pode ser definido como

\begin{equation} 
  \sigma_{n}^{2} = \omega + \alpha\mu_{n-1}^{2} + \beta\sigma_{n-1}^{2}
  \label{eq:garch2}
\end{equation}

A equa√ß√£o \eqref{eq:garch2} √© a que costuma ser utilizada de forma a estimar os par√¢metros. De notar de que para obter um processo est√°vel GARCH(1,1) √© necess√°rio que \(\sigma+\beta<1\), ou ent√£o o peso aplicado a vari√¢ncia de longo prazo √© negativa.

A estima√ß√£o dos par√¢metros, tanto para o modelo EWMA como para o modelo GARCH(1,1), √© realizado utilizando um processo conhecido como \emph{maximum likelihood method}, onde se utilizam os dados hist√≥ricos de modo a escolher os par√¢metros que maximizem a probabilidade de ocorr√™ncia dos dados, de acordo com a equa√ß√£o

\begin{equation} 
  \sum_{i=1}^{m}=\bigg[-ln(\sigma_{i}^{2})-\frac{\mu_{i}^{2}}{\sigma_{i}^{2}}\bigg]
  \label{eq:mgarch}
\end{equation}

\hypertarget{volatilidade-impluxedcita}{%
\subsection{Volatilidade impl√≠cita}\label{volatilidade-impluxedcita}}

O modelo de Black-Scholes-Merton\footnote{Os autores, Robert Merton e Myron Scholes, foram reconhecidos com o Nobel da economia em 1997} foi um modelo desenvolvido nos anos 70, considerado um ponto de ruptura na defini√ß√£o do pre√ßo de op√ß√µes sobre ac√ß√µes europeias.

De entre os pressupostos\footnote{Para mais informa√ß√£o ver \citet{BlackScholes},pp.~640, capitulo ``The valuation formula''} utilizados para derivar a equa√ß√£o diferencial, importa salientar de que a taxa de juro sem risco, r, √© constante e a mesma ao longo do tempo. De acordo com \citet{Hull2018} ``√â importante considerar que a avalia√ß√£o sem risco (ou o pressuposto de que todos os investidores s√£o neutros ao risco) √© meramente um artif√≠cio para obter solu√ß√£o para a equa√ß√£o diferencial de Black-Scholes-Merton''(p.312).

O processo que se assume ser aplicado ao pre√ßo dos activos √© o descrito pela equa√ß√£o \eqref{eq:ito}. De salientar de que nenhumas das vari√°veis da equa√ß√£o √© afectada pelas prefer√™ncias de risco dos investidores, sendo que as vari√°veis que aparecem na equa√ß√£o s√£o o valor presente do activo financeiro, tempo, volatilidade e a taxa de juro sem risco.

As solu√ß√µes obtidas a partir da equa√ß√£o diferenci√°vel de interesse para calculo da volatilidade impl√≠cita, s√£o as aqui apresentadas para valorizar op√ß√µes de compra (\emph{call}) e op√ß√µes de venda (\emph{put}) do tipo europeias:

\begin{equation} 
  c = S_0N(d_1) - Ke^{-rT}N(d_2);
  \label{eq:call}
\end{equation}
e
\begin{equation} 
  p = Ke^{-rT}N(-d_2) - S_0N(-d_1);
  \label{eq:put}
\end{equation}
onde
\begin{equation} 
  d_1 = \frac{In(S_0/K)+(r+\sigma^2/2)T}{\sigma\sqrt{T}}
  \label{eq:d1}
\end{equation}
\begin{equation} 
  d_2 = \frac{In(S_0/K)+(r-\sigma^2/2)T}{\sigma\sqrt{T}}=d_1-\sigma\sqrt{T}
  \label{eq:d2}
\end{equation}

A fun√ß√£o N(x) representa a fun√ß√£o distribui√ß√£o para uma distribui√ß√£o normal padr√£o, sendo a probabilidade de uma vari√°vel ser menor que x (ver exemplo na figura \ref{fig:fdistribuicao}). O valor de N(x) pode ser obtido com a fun√ß√£o do R pnorm(). A vari√°vel T representa o tempo medido em dias de negocia√ß√£o que faltam at√© a expira√ß√£o da op√ß√£o dividido pelos dias de negocia√ß√£o nesse ano, sendo S\textsubscript{0} o valor do activo subjacente no tempo 0, r a taxa de juro sem risco, \(\sigma\) a volatilidade anual e K o pre√ßo de exerc√≠cio da op√ß√£o.

\begin{figure}

{\centering \includegraphics[width=0.45\linewidth]{image/fdistribuicao} 

}

\caption{Fun√ß√£o distribui√ß√£o N(x)}\label{fig:fdistribuicao}
\end{figure}
\centering

Fonte: Elabora√ß√£o pr√≥pria.

\justifying
\bigskip

O par√¢metro da volatilidade n√£o √© um par√¢metro que se consiga obter ou observar directamente, sendo um valor que representa a volatilidade esperada pelos investidores no futuro, conhecido como \emph{volatilidade impl√≠cita}. Um modo de obter este valor √© a partir dos restantes par√¢metros, ou seja, tendo o valor de uma op√ß√£o de compra, e considerando os valores de S\textsubscript{0}, K, r e T, substituir estes pelos respectivos valores e obter o valor de \(\sigma\) que iguale o valor da \emph{call} utilizando as equa√ß√µes \eqref{eq:call} e \eqref{eq:d1} atrav√©s de processos por aproxima√ß√£o iterativo.

A volatilidade impl√≠cita ser√° diferente consoante o pre√ßo de exerc√≠cio da op√ß√£o, considerando todos os outros par√¢metros iguais. Da conjuga√ß√£o dos valore de volatilidade obtidos para cada um dos pre√ßos de exerc√≠cio da op√ß√£o obt√™m-se um gr√°fico conhecido como \emph{volatility smile}(figura \ref{fig:volatilitysmile}). Este gr√°fico ser√° o mesmo, quer se esteja a considerar uma op√ß√£o do tipo \emph{call} ou a mesma equivalente op√ß√£o do tipo \emph{put}.

A vari√°vel volatilidade, ùúé, √© definida como o desvio padr√£o do retorno fornecido pela
vari√°vel por unidade de tempo quando o retorno √© expresso usando juros compostos.
Quando a volatilidade √© usada para valorizar op√ß√µes, a unidade de tempo √© normalmente 1 ano, sendo a volatilidade o desvio padr√£o do retorno compostos por ano.
Quando a volatilidade √© usada para a gest√£o de risco, a unidade de tempo usual √© 1 dia
de forma a que a volatilidade √© o desvio padr√£o do retorno composto por dia.''(pp.213-
214)

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{image/volatilitysmile} 

}

\caption{Volatility Smile}\label{fig:volatilitysmile}
\end{figure}
\centering

Fonte: \citep[pp.182]{volatilitysmile}

\justifying
\bigskip

Como de pode ver pela figura \ref{fig:volatilitysmile} a op√ß√£o √© considerada \emph{out-the-money} se o seu valor for inferior ao do respectivo subjacente, \emph{at-the-money} se for superior e \emph{in-the-money} se o valor for igual. No calculo do valor da volatilidade ser√° utilizado o valor de pre√ßo de exerc√≠cio \emph{in-the-money} de op√ß√µes do tipo \emph{call}, ou na impossibilidade do valor do subjacente ser o mesmo do K, o mais pr√≥ximo deste.

Quando comparado com outros m√©todos para calcular a volatilidade onde os dados utilizados s√£o dados hist√≥ricos, este m√©todo incorpora o sentimento presente dos investidores relativo a volatilidade futura de um determinado ativo ou instrumento financeiro.

\hypertarget{portfolios-muxe9dia-variuxe2ncia}{%
\section{Portfolios m√©dia-vari√¢ncia}\label{portfolios-muxe9dia-variuxe2ncia}}

A otimiza√ß√£o de portfolios atrav√©s da diversifica√ß√£o √© um conceito b√°sico que teve origem em Markowitz, criando o conceito de fronteira eficiente. Existem v√°rios pressupostos definidos na obten√ß√£o deste modelo, n√£o sendo, no entanto, o √¢mbito aqui analisar esses mesmos pressupostos, considerando que independentemente disso esses mesmos pressupostos s√£o verificados. De acordo com \citet{Modern2013}, ``todas os pressupostos acerca da analise de portfolio foram demonstradas serem simplistas, e em alguns casos demasiado simplistas.{[}\ldots{]}Pessoas necessitam apenas de se comportar como se fossem descritas pelos pressupostos para uma teoria ser v√°lida'' \citep[pp.5]{Modern2013}.

Para aplica√ß√£o deste modelo deve-se obter os seguintes dados dos instrumentos financeiros que v√£o constituir o portfolio:

\begin{itemize}
\tightlist
\item
  A taxa do retorno esperado, E(r);
\item
  O desvio padr√£o dos retornos, \(\sigma\);
\item
  O coeficiente de correla√ß√£o,\(\rho\), entre cada um dos activos.
\end{itemize}

O retorno esperado vai depender de v√°rios factores, essencialmente a taxa de retorno sem risco que se verifica no mercado, a taxa de infla√ß√£o e o risco a que o investidor estar√° sujeito, sendo que quanto maior o risco, maior ser√° o retorno esperado.
As s√©ries dos retornos di√°rios para cada um dos activos √© calculada de acordo com a seguinte formula:

\begin{equation} 
  R_i = ln\Big(\frac{S_f}{S_i}\Big)
  \label{eq:logRet}
\end{equation}

A m√©dia dos retornos de cada um dos activos √© calculada de acordo com a sua m√©dia aritm√©tica:

\begin{equation} 
  \overline{R} = \frac{\displaystyle\sum_{i=1}^n R_i}{n}
  \label{eq:meanRet}
\end{equation}

O desvio padr√£o representa a volatilidade, ou risco, associada ao activo. Considerando que essa volatilidade √© calculada com base nos dados hist√≥ricos e as formulas acima, ent√£o a estat√≠stica da amostra para o desvio padr√£o representa-se pela seguinte formulas:

\begin{equation} 
  \hat{\sigma} = \sqrt\frac{\displaystyle\sum_{i=1}^n (R_i-\overline{R})^2}{n-1}
  \label{eq:estdesviopadrao}
\end{equation}

A escolha do tamanho da amostra (n) deve ser grande o suficiente de modo a garantir uma melhor precis√£o nas estat√≠sticas obtidas, sendo que neste caso devemos considerar que a volatilidade n√£o √© constante ao longo do tempo e que valores mais antigos podem n√£o ser t√£o relevantes como os valores mais recentes. De acordo com \citep{Hull2018} esse valor deve ser compreendido entre 90 a 180 dias para a cota√ß√£o de activos.

Para o calculo da volatilidade anual utiliza-se os n√∫mero de dias de negocia√ß√£o, sendo considerado 252 dias por ano como valor de refer√™ncia.

\hypertarget{value-at-risk}{%
\section{Value at Risk}\label{value-at-risk}}

A quantifica√ß√£o do risco √© fundamental para a gest√£o e atenua√ß√£o de perdas no valor da carteira ao longo do tempo. Na gest√£o da carteira a aplica√ß√£o de m√©tricas de risco permite a adapta√ß√£o cont√≠nua do portf√≥lio aos fatores de risco quantific√°veis, podendo-se, atrav√©s de uma gest√£o ativa, manter o investidor devidamente informado relativamente ao risco que o seu investimento incorpora e poder tomar medidas proactivas de adapta√ß√£o do investimento ao n√≠vel de risco verificado, tendo em considera√ß√£o o perfil do investidor.O Value at Risk (VaR) √© uma dessas m√©tricas de risco.

O Value at Risk (VaR)\footnote{Markowitz foi dos primeiros autores a contemplar a an√°lise de risco em investimentos, como se pode verificar em \citet{Markowitz1952}} √© uma medida probabil√≠stica para a perda m√°xima prov√°vel de uma carteira para um n√≠vel de confian√ßa determinado,num horizonte temporal especificado. Pode-se definir o VaR como ``descrevendo o quantil da distribui√ß√£o de ganhos e perdas projectado ao longo do horizonte alvo. Se \emph{c} for o intervalo de confian√ßa selecionado, VaR corresponde ao n√≠vel inferior da cauda 1-\emph{c}''\citep[pp.17]{philippe}. Este valor √© sempre positivo.

O calculo do valor do VaR √© de car√°cter obrigat√≥rio para entidades financeiras e seguradoras, sendo que os reguladores, dependendo da sua jurisdi√ß√£o, determinam os par√¢metros quantitativos a serem utilizados.
No caso do Banco Central Europeu\footnote{ver relat√≥rio t√©cnico \citet{ecb}}, esses par√¢metros s√£o de que o c√°lculo deve ser efectuado para um intervalo temporal de 10 dias de negocia√ß√£o e para um intervalo de confian√ßa de 99\%, sendo considerado para esses c√°lculos pelo menos 250 dias negocia√ß√£o de observa√ß√£o de valores hist√≥ricos.

O c√°lculo do VaR pode ser realizado de modos diferentes, existindo v√°rios m√©todos, sendo considerados m√©todos de tipo n√£o param√©trico, como o m√©todo hist√≥rico em que n√£o se assume nenhum pressuposto na distribui√ß√£o desses dados, ou m√©todos do tipo param√©trico, onde se assume um determinado tipo de distribui√ß√£o dos dados. Para efeito desta disserta√ß√£o ser√° aplicado um m√©todo param√©trico para c√°lculo do VaR.

\hypertarget{var-paramuxe9trico-gaussiana}{%
\subsection{VaR Param√©trico (Gaussiana)}\label{var-paramuxe9trico-gaussiana}}

O VaR param√©trico pressup√µe que os retornos di√°rios apresentem uma distribui√ß√£o normal (figura \ref{fig:var}), sendo que a sua representa√ß√£o matem√°tica se encontra definida na equa√ß√£o \eqref{eq:var}.
\begin{equation} 
  VaR_{t+1}^{p} = -\sigma_{t+1}\Phi_{p}^{-1}
  \label{eq:var}
\end{equation}


\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{image/VaR} 

}

\caption{VaR com intervalo confian√ßa 95\% e 99\%.}\label{fig:var}
\end{figure}
\FloatBarrier
\centering

Fonte: \citep[pp.13]{phdthesis}

\justifying
\bigskip

O \(\Phi (p)\) representa a fun√ß√£o distribui√ß√£o e \(\Phi^{-1} (p)\) a sua inversa, sendo que, por exemplo, para um intervalo de confian√ßa de 99\% o valor de \(\Phi^{-1} (p)\) corresponde a -2.33. Se considerarmos que a volatilidade prevista a 1 dia √© de 2\%, teremos um VaR = -0.020*(-2.33), o que corresponde a um valor de 0.0466. Podemos interpretar o VaR como sendo a exist√™ncia de 1\% de probabilidade de perder mais do que 4.66\% do valor investido no activo no dia de hoje.

O \emph{Daily Value at Risk} (DEaR) √© o valor diariamente em risco, sendo calculado de acordo com a equa√ß√£o \eqref{eq:dear}.
\begin{equation} 
  ‚Ç¨DEaR = ‚Ç¨ \textnormal{Valor de mercado do investimento} * VaR
  \label{eq:dear}  
\end{equation}

O valor do VaR a mais de um dia pode-se calcular a partir do DEaR a um dia, de acordo com a equa√ß√£o \eqref{eq:vardays}, onde N representa o n√∫mero de dias.
\begin{equation} 
  VaR = DEaR*\sqrt{N}
  \label{eq:vardays}
\end{equation}

Se estivermos a analisar o VaR de um portfolio, esse √© calculado da mesma forma que a vari√¢ncia de um portfolio (equa√ß√£o \eqref{eq:varport}), considerando o DEaR individual de cada componente.
\begin{equation} 
  DEaR_{portfolio}^{2} = \sum_{i=1}^{N}\sum_{j=1}^{N}Cov(DEaR_i,DEaR_j)
  \label{eq:varport}
\end{equation}
onde
\begin{equation} 
  Cov(DEaR_i,DEaR_j) = DEaR_{i}DEaR_{j}\rho(DEaR_i,DEaR_j)
  \label{eq:covport}
\end{equation}

Por exemplo, considerando 2 activos, A e B, constituintes de um portfolio, ter√≠amos:
\begin{equation} 
  DEaR_{A+B}^{2} = DEar_{A}^{2}+DEaR_{B}^{2}+2\rho_{AB} DEaR_{A} DEaR_B
  \label{eq:ab}
\end{equation}

\hypertarget{var-paramuxe9trico-aproximauxe7uxe3o-cornish-fisher}{%
\subsection{VaR Param√©trico (Aproxima√ß√£o Cornish-Fisher)}\label{var-paramuxe9trico-aproximauxe7uxe3o-cornish-fisher}}

Quando a distribui√ß√£o dos retornos apresentam excesso de curtose\footnote{medida de dispers√£o que caracteriza o achatamento da curva da fun√ß√£o de distribui√ß√£o} (\emph{Kurtosis}) e/ou assimetria\footnote{permite distinguir as distribui√ß√µes assim√©tricas} (\emph{Skewness}) relativamente a uma distribui√ß√£o normal pode-se proceder √† aproxima√ß√£o Cornish-Fisher, permitindo desta forma uma aproxima√ß√£o ao VaR. Por exemplo, na figura \ref{fig:quant} temos uma distribui√ß√£o do tipo leptoc√∫rtica, sendo caracterizada por um pico mais alto e caudas mais pesadas que uma distribui√ß√£o normal.
\bigskip

\begin{figure}

{\centering \includegraphics[width=0.6\linewidth]{image/kurtosis} 

}

\caption{Kurtosis}\label{fig:quant}
\end{figure}
\centering

Fonte: \citep[pp.46]{quant}

\justifying

O c√°lculo do VaR com aproxima√ß√£o Cornish-Fisher efectua-se de acordo com a equa√ß√£o \eqref{eq:varcf}, sendo que para calcular o valor referente a CF se aplica a equa√ß√£o \eqref{eq:cf}.
\begin{equation} 
  VaR_{t+1}^{p} = -\sigma_{t+1}*CF_{p}^{-1}
  \label{eq:varcf}
\end{equation}
onde
\begin{equation} 
  CF_{p}^{-1} = \Phi_{p}^{-1} + \frac{\zeta_{1}}{6}[(\Phi_{p}^{-1})^2-1] + \frac{\zeta_{2}}{24}[(\Phi_{p}^{-1})^3-3\Phi_{p}^{-1}] - \frac{\zeta_{1}^{2}}{36}[2(\Phi_{p}^{-1})^3-5\Phi_{p}^{-1}]
  \label{eq:cf}
\end{equation}

Os par√¢metros referentes a \(\zeta_1\) e \(\zeta_2\) s√£o respectivamente os valores da assimetria e do excesso de curtose. Esses valores s√£o obtidos de acordo com as equa√ß√µes \eqref{eq:assimetria} e \eqref{eq:curtose}.
\begin{equation} 
 \zeta_1 = \frac{1}{n}\sum\bigg[\frac{X_i - \overline{X}}{\sigma}\bigg]^3
  \label{eq:assimetria}
\end{equation}
\begin{equation} 
 \zeta_2 = \frac{1}{n}\sum\bigg[\frac{X_i - \overline{X}}{\sigma}\bigg]^4 - 3
  \label{eq:curtose}
\end{equation}

\hypertarget{pacotes-do-r-para-anuxe1lise}{%
\chapter{Pacotes do R para an√°lise}\label{pacotes-do-r-para-anuxe1lise}}

\newpage

We describe our methods in this chapter.

\hypertarget{aplicauxe7uxe3o-a-dados-do-modelo}{%
\chapter{Aplica√ß√£o a dados do modelo}\label{aplicauxe7uxe3o-a-dados-do-modelo}}

\endgroup
\newpage

\hypertarget{aplicauxe7uxe3o-a-dados-do-modelo-1}{%
\section{Aplica√ß√£o a dados do modelo}\label{aplicauxe7uxe3o-a-dados-do-modelo-1}}

A primeira fase consiste em extrair de forma aleat√≥ria 14 ac√ß√µes constituintes do Euro Stoxx 50, estando os valores obtidos representados na tabela \ref{tab:nice-tab}.
\scriptsize

\begin{table}[!h]

\caption{\label{tab:nice-tab}Empresas extraidas do Euro Stoxx 50}
\centering
\begin{tabular}[t]{l}
\toprule
x\\
\midrule
UNILEVER ORD\\
SOCIETE GENERALE ORD\\
BAYER N ORD\\
TELEFONICA ORD\\
ENEL ORD\\
\addlinespace
DEUTSCHE TELEKOM N ORD\\
AB INBEV ORD\\
DAIMLER N ORD\\
ORANGE ORD\\
SANOFI ORD\\
\addlinespace
AIRBUS ORD\\
INTESA SANPAOLO ORD\\
AHOLD DEL ORD\\
ASML HOLDING ORD\\
\bottomrule
\end{tabular}
\end{table}

\normalsize

Reference a figure by its code chunk label with the \texttt{fig:} prefix, e.g., see Figure \ref{fig:nice-fig}.
\scriptsize

\begin{figure}

{\centering \includegraphics[width=0.3\linewidth]{bookdown-demo_files/figure-latex/nice-fig-1} 

}

\caption{Here is a nice figure!}\label{fig:nice-fig}
\end{figure}

Calculo do retorno diario logaritmico
\scriptsize

\normalsize

Correla√ß√£o entre activos
\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#do not print hash}
\CommentTok{\#options(width = 70)}
\CommentTok{\#cor(returns)}
\end{Highlighting}
\end{Shaded}

\normalsize

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#par(mar = c(1,1,1,1))}
\CommentTok{\#plot(returns$TELEFONICA, main = "TEF.MC",col="red")}
\CommentTok{\#plot(returns$UNILEVER, main = "UNA.AS",col="red")}
\CommentTok{\#plot(returns$SOCGEN, main = "GLE.PA",col="red")}
\CommentTok{\#plot(returns$BAYER, main = "ENEL.MI",col="red")}
\CommentTok{\#plot(returns$ENEL, main = "BAYN.DE",col="red")}
\CommentTok{\#plot(returns$\textasciigrave{}DEUTSCHE TELEKOM\textasciigrave{}, main = "DTE.DE",col="red")}
\CommentTok{\#plot(returns$\textasciigrave{}AB INBEV\textasciigrave{}, main = "ABI.BR",col="red")}
\CommentTok{\#plot(returns$DAIMLER, main = "DAI.DE",col="red")}
\CommentTok{\#plot(returns$ORANGE, main = "ORA.PA",col="red")}
\CommentTok{\#plot(returns$SANOFI, main = "SAN.PA",col="red")}
\CommentTok{\#plot(returns$AIRBUS, main = "AIR.PA",col="red")}
\CommentTok{\#plot(returns$INTESA, main = "ISP.MI",col="red")}
\CommentTok{\#plot(returns$AHOLD, main = "AD.AS",col="red")}
\CommentTok{\#plot(returns$ASML, main = "ASML.AS",col="red")}
\end{Highlighting}
\end{Shaded}

\normalsize

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
 \CommentTok{\#par(mar = c(2,2,2,2))}
 \CommentTok{\#hist(returns$TELEFONICA,probability=T, main="TELEFONICA {-} daily.}
      \CommentTok{\#returns",xlab="Approximately normally distributed data",breaks=100)}
 \CommentTok{\#lines(density(na.omit(returns$TELEFONICA)),col=2)}
 \CommentTok{\#curve(dnorm(x,0,0.01640906), from = {-}0.15,to=0.15, col=\textquotesingle{}blue\textquotesingle{},add = TRUE)}
 \CommentTok{\#qqnorm(returns$TELEFONICA,main="QQ plot of normal data",pch=19)}
 \CommentTok{\#qqline(returns$TELEFONICA)}

\NormalTok{return\textless{}{-}return[}\OperatorTok{{-}}\DecValTok{1}\NormalTok{]}
\KeywordTok{hist}\NormalTok{(return,}\DataTypeTok{breaks=}\DecValTok{100}\NormalTok{)}
\KeywordTok{chart.Histogram}\NormalTok{(return,}\DataTypeTok{methods =} \KeywordTok{c}\NormalTok{(}\StringTok{"add.density"}\NormalTok{,}\StringTok{"add.normal"}\NormalTok{),}\DataTypeTok{colorset =} \KeywordTok{c}\NormalTok{(}\StringTok{"blue"}\NormalTok{,}\StringTok{"green"}\NormalTok{,}\StringTok{"red"}\NormalTok{))}
\KeywordTok{chartSeries}\NormalTok{(return,}\DataTypeTok{theme=}\StringTok{"white"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.3\linewidth]{bookdown-demo_files/figure-latex/unnamed-chunk-7-1} \includegraphics[width=0.3\linewidth]{bookdown-demo_files/figure-latex/unnamed-chunk-7-2} \includegraphics[width=0.3\linewidth]{bookdown-demo_files/figure-latex/unnamed-chunk-7-3} \end{center}
\normalsize

\scriptsize

\begin{Shaded}
\begin{Highlighting}[]
 \CommentTok{\#TELTestNOR \textless{}{-} shapiro.test(as.numeric(returns$TELEFONICA))}
 \CommentTok{\#TELSha \textless{}{-} matrix(c(TELTestNOR$statistic,TELTestNOR$p.value),ncol = 2)}
 \CommentTok{\#colnames(TELSha) \textless{}{-} c("Estatistica","p{-}value")}
 \CommentTok{\#rownames(TELSha) \textless{}{-} c("Shapiro{-}Wilk normality test")}
 \CommentTok{\#TELSha \textless{}{-} as.table(TELSha)}
 \CommentTok{\#d \textless{}{-} knitr::kable(}
  \CommentTok{\#TELSha, caption = \textquotesingle{}Empresas extraidas do Euro Stoxx 50\textquotesingle{},}
  \CommentTok{\#booktabs = TRUE}
\CommentTok{\#)}
\CommentTok{\#kable\_styling(d, latex\_options = "hold\_position", position = "center")}
\end{Highlighting}
\end{Shaded}

\normalsize

Annualized volatility
\scriptsize

\normalsize

Como se pode verificar, o desvio padr√£o da empresa telecomunica√ß√£o ``Telefonica'' √©

sGARCH model with contant mean

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{s\textless{}{-}}\KeywordTok{ugarchspec}\NormalTok{(}\DataTypeTok{mean.model =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{armaOrder=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{)),}\DataTypeTok{variance.model =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{model=}\StringTok{"sGARCH"}\NormalTok{),}
              \DataTypeTok{distribution.model =} \StringTok{"norm"}\NormalTok{)}
\NormalTok{m\textless{}{-}}\KeywordTok{ugarchfit}\NormalTok{(}\DataTypeTok{data=}\NormalTok{return,}\DataTypeTok{spec=}\NormalTok{s)}
\NormalTok{m}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## *---------------------------------*
## *          GARCH Model Fit        *
## *---------------------------------*
## 
## Conditional Variance Dynamics    
## -----------------------------------
## GARCH Model  : sGARCH(1,1)
## Mean Model   : ARFIMA(0,0,0)
## Distribution : norm 
## 
## Optimal Parameters
## ------------------------------------
##         Estimate  Std. Error  t value Pr(>|t|)
## mu      0.000389    0.000392  0.99187  0.32126
## omega   0.000004    0.000002  1.71328  0.08666
## alpha1  0.075375    0.010477  7.19431  0.00000
## beta1   0.922271    0.010824 85.20606  0.00000
## 
## Robust Standard Errors:
##         Estimate  Std. Error  t value Pr(>|t|)
## mu      0.000389    0.000419  0.92904 0.352871
## omega   0.000004    0.000009  0.46954 0.638681
## alpha1  0.075375    0.035773  2.10704 0.035115
## beta1   0.922271    0.039301 23.46692 0.000000
## 
## LogLikelihood : 5059.169 
## 
## Information Criteria
## ------------------------------------
##                     
## Akaike       -4.9805
## Bayes        -4.9694
## Shibata      -4.9805
## Hannan-Quinn -4.9764
## 
## Weighted Ljung-Box Test on Standardized Residuals
## ------------------------------------
##                         statistic   p-value
## Lag[1]                      11.48 0.0007035
## Lag[2*(p+q)+(p+q)-1][2]     11.73 0.0006672
## Lag[4*(p+q)+(p+q)-1][5]     12.13 0.0025719
## d.o.f=0
## H0 : No serial correlation
## 
## Weighted Ljung-Box Test on Standardized Squared Residuals
## ------------------------------------
##                         statistic p-value
## Lag[1]                      1.074  0.3001
## Lag[2*(p+q)+(p+q)-1][5]     2.189  0.5744
## Lag[4*(p+q)+(p+q)-1][9]     3.741  0.6330
## d.o.f=2
## 
## Weighted ARCH LM Tests
## ------------------------------------
##             Statistic Shape Scale P-Value
## ARCH Lag[3]   0.08693 0.500 2.000  0.7681
## ARCH Lag[5]   2.35507 1.440 1.667  0.3980
## ARCH Lag[7]   3.09298 2.315 1.543  0.4962
## 
## Nyblom stability test
## ------------------------------------
## Joint Statistic:  1.4582
## Individual Statistics:              
## mu     0.42569
## omega  0.24745
## alpha1 0.11803
## beta1  0.06953
## 
## Asymptotic Critical Values (10% 5% 1%)
## Joint Statistic:          1.07 1.24 1.6
## Individual Statistic:     0.35 0.47 0.75
## 
## Sign Bias Test
## ------------------------------------
##                    t-value   prob sig
## Sign Bias           0.3640 0.7159    
## Negative Sign Bias  1.2678 0.2050    
## Positive Sign Bias  0.7081 0.4790    
## Joint Effect        2.1270 0.5465    
## 
## 
## Adjusted Pearson Goodness-of-Fit Test:
## ------------------------------------
##   group statistic p-value(g-1)
## 1    20     71.95    4.346e-08
## 2    30    103.28    2.901e-10
## 3    40     97.57    6.354e-07
## 4    50    115.42    2.788e-07
## 
## 
## Elapsed time : 0.1551001
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(m,}\DataTypeTok{which =} \StringTok{"all"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## please wait...calculating quantiles...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f\textless{}{-}}\KeywordTok{ugarchforecast}\NormalTok{(}\DataTypeTok{fitORspec =}\NormalTok{ m,}\DataTypeTok{n.ahead =} \DecValTok{60}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{fitted}\NormalTok{(f))}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{sigma}\NormalTok{(f))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-10-1} \includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-10-2} \includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-10-3} \end{center}

Application example - potfolio allocation

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{v\textless{}{-}}\KeywordTok{sqrt}\NormalTok{(}\DecValTok{252}\NormalTok{)}\OperatorTok{*}\KeywordTok{sigma}\NormalTok{(m)}
\NormalTok{w\textless{}{-}}\FloatTok{0.05}\OperatorTok{/}\NormalTok{v}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{merge}\NormalTok{(v,w),}\DataTypeTok{multi.panel=}\OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-11-1} \end{center}

GARCH with sstd

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{s\textless{}{-}}\KeywordTok{ugarchspec}\NormalTok{(}\DataTypeTok{mean.model =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{armaOrder=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{)),}\DataTypeTok{variance.model =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{model=}\StringTok{"sGARCH"}\NormalTok{),}
              \DataTypeTok{distribution.model =} \StringTok{"sstd"}\NormalTok{)}
\NormalTok{m\textless{}{-}}\KeywordTok{ugarchfit}\NormalTok{(}\DataTypeTok{data=}\NormalTok{return,}\DataTypeTok{spec=}\NormalTok{s)}
\NormalTok{m}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## *---------------------------------*
## *          GARCH Model Fit        *
## *---------------------------------*
## 
## Conditional Variance Dynamics    
## -----------------------------------
## GARCH Model  : sGARCH(1,1)
## Mean Model   : ARFIMA(0,0,0)
## Distribution : sstd 
## 
## Optimal Parameters
## ------------------------------------
##         Estimate  Std. Error  t value Pr(>|t|)
## mu      0.000255    0.000391  0.65304 0.513731
## omega   0.000005    0.000004  1.36064 0.173627
## alpha1  0.064102    0.013708  4.67619 0.000003
## beta1   0.929022    0.015711 59.13126 0.000000
## skew    1.019436    0.031535 32.32671 0.000000
## shape   5.178852    0.573798  9.02557 0.000000
## 
## Robust Standard Errors:
##         Estimate  Std. Error  t value Pr(>|t|)
## mu      0.000255    0.000402  0.63445 0.525784
## omega   0.000005    0.000009  0.53828 0.590383
## alpha1  0.064102    0.031607  2.02807 0.042553
## beta1   0.929022    0.038615 24.05869 0.000000
## skew    1.019436    0.028874 35.30686 0.000000
## shape   5.178852    0.637241  8.12699 0.000000
## 
## LogLikelihood : 5153.861 
## 
## Information Criteria
## ------------------------------------
##                     
## Akaike       -5.0718
## Bayes        -5.0552
## Shibata      -5.0718
## Hannan-Quinn -5.0657
## 
## Weighted Ljung-Box Test on Standardized Residuals
## ------------------------------------
##                         statistic   p-value
## Lag[1]                      10.87 0.0009800
## Lag[2*(p+q)+(p+q)-1][2]     11.16 0.0009447
## Lag[4*(p+q)+(p+q)-1][5]     11.63 0.0034717
## d.o.f=0
## H0 : No serial correlation
## 
## Weighted Ljung-Box Test on Standardized Squared Residuals
## ------------------------------------
##                         statistic p-value
## Lag[1]                      1.989  0.1584
## Lag[2*(p+q)+(p+q)-1][5]     2.966  0.4133
## Lag[4*(p+q)+(p+q)-1][9]     5.037  0.4241
## d.o.f=2
## 
## Weighted ARCH LM Tests
## ------------------------------------
##             Statistic Shape Scale P-Value
## ARCH Lag[3]  0.006045 0.500 2.000  0.9380
## ARCH Lag[5]  2.652356 1.440 1.667  0.3443
## ARCH Lag[7]  3.804502 2.315 1.543  0.3756
## 
## Nyblom stability test
## ------------------------------------
## Joint Statistic:  3.5779
## Individual Statistics:              
## mu     0.27077
## omega  0.36609
## alpha1 0.20932
## beta1  0.40154
## skew   0.04104
## shape  0.54690
## 
## Asymptotic Critical Values (10% 5% 1%)
## Joint Statistic:          1.49 1.68 2.12
## Individual Statistic:     0.35 0.47 0.75
## 
## Sign Bias Test
## ------------------------------------
##                    t-value   prob sig
## Sign Bias           0.3186 0.7500    
## Negative Sign Bias  1.6346 0.1023    
## Positive Sign Bias  1.1215 0.2622    
## Joint Effect        3.9400 0.2680    
## 
## 
## Adjusted Pearson Goodness-of-Fit Test:
## ------------------------------------
##   group statistic p-value(g-1)
## 1    20     27.79      0.08749
## 2    30     32.17      0.31257
## 3    40     55.16      0.04474
## 4    50     49.75      0.44312
## 
## 
## Elapsed time : 0.7176251
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(m,}\DataTypeTok{which =} \StringTok{"all"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## please wait...calculating quantiles...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f\textless{}{-}}\KeywordTok{ugarchforecast}\NormalTok{(}\DataTypeTok{fitORspec =}\NormalTok{ m,}\DataTypeTok{n.ahead =} \DecValTok{60}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{fitted}\NormalTok{(f))}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{sigma}\NormalTok{(f))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-12-1} \includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-12-2} \includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-12-3} \end{center}

GJR-GARCH

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{s\textless{}{-}}\KeywordTok{ugarchspec}\NormalTok{(}\DataTypeTok{mean.model =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{armaOrder=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{)),}\DataTypeTok{variance.model =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{model=}\StringTok{"gjrGARCH"}\NormalTok{),}
              \DataTypeTok{distribution.model =} \StringTok{"sstd"}\NormalTok{)}
\NormalTok{m\textless{}{-}}\KeywordTok{ugarchfit}\NormalTok{(}\DataTypeTok{data=}\NormalTok{return,}\DataTypeTok{spec=}\NormalTok{s)}
\NormalTok{m}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## *---------------------------------*
## *          GARCH Model Fit        *
## *---------------------------------*
## 
## Conditional Variance Dynamics    
## -----------------------------------
## GARCH Model  : gjrGARCH(1,1)
## Mean Model   : ARFIMA(0,0,0)
## Distribution : sstd 
## 
## Optimal Parameters
## ------------------------------------
##         Estimate  Std. Error  t value Pr(>|t|)
## mu      0.000122    0.000388  0.31361 0.753820
## omega   0.000006    0.000004  1.39140 0.164104
## alpha1  0.029608    0.010561  2.80351 0.005055
## beta1   0.921084    0.015405 59.79096 0.000000
## gamma1  0.084912    0.023364  3.63435 0.000279
## skew    1.018822    0.031592 32.24906 0.000000
## shape   5.320881    0.598556  8.88952 0.000000
## 
## Robust Standard Errors:
##         Estimate  Std. Error  t value Pr(>|t|)
## mu      0.000122    0.000409  0.29715 0.766350
## omega   0.000006    0.000010  0.57529 0.565099
## alpha1  0.029608    0.011326  2.61409 0.008947
## beta1   0.921084    0.031316 29.41235 0.000000
## gamma1  0.084912    0.041527  2.04476 0.040878
## skew    1.018822    0.030441 33.46821 0.000000
## shape   5.320881    0.694823  7.65789 0.000000
## 
## LogLikelihood : 5164.008 
## 
## Information Criteria
## ------------------------------------
##                     
## Akaike       -5.0808
## Bayes        -5.0614
## Shibata      -5.0808
## Hannan-Quinn -5.0737
## 
## Weighted Ljung-Box Test on Standardized Residuals
## ------------------------------------
##                         statistic  p-value
## Lag[1]                      10.79 0.001022
## Lag[2*(p+q)+(p+q)-1][2]     11.01 0.001035
## Lag[4*(p+q)+(p+q)-1][5]     11.34 0.004136
## d.o.f=0
## H0 : No serial correlation
## 
## Weighted Ljung-Box Test on Standardized Squared Residuals
## ------------------------------------
##                         statistic p-value
## Lag[1]                      1.161  0.2813
## Lag[2*(p+q)+(p+q)-1][5]     2.869  0.4315
## Lag[4*(p+q)+(p+q)-1][9]     4.165  0.5612
## d.o.f=2
## 
## Weighted ARCH LM Tests
## ------------------------------------
##             Statistic Shape Scale P-Value
## ARCH Lag[3]    0.5849 0.500 2.000  0.4444
## ARCH Lag[5]    2.4806 1.440 1.667  0.3745
## ARCH Lag[7]    2.9191 2.315 1.543  0.5290
## 
## Nyblom stability test
## ------------------------------------
## Joint Statistic:  2.3582
## Individual Statistics:              
## mu     0.56969
## omega  0.73784
## alpha1 0.57993
## beta1  0.87042
## gamma1 0.64879
## skew   0.02895
## shape  0.98210
## 
## Asymptotic Critical Values (10% 5% 1%)
## Joint Statistic:          1.69 1.9 2.35
## Individual Statistic:     0.35 0.47 0.75
## 
## Sign Bias Test
## ------------------------------------
##                    t-value   prob sig
## Sign Bias           0.2050 0.8376    
## Negative Sign Bias  0.4205 0.6741    
## Positive Sign Bias  1.5051 0.1324    
## Joint Effect        3.3281 0.3438    
## 
## 
## Adjusted Pearson Goodness-of-Fit Test:
## ------------------------------------
##   group statistic p-value(g-1)
## 1    20     15.81       0.6697
## 2    30     29.86       0.4209
## 3    40     40.94       0.3855
## 4    50     60.89       0.1187
## 
## 
## Elapsed time : 1.250843
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(m,}\DataTypeTok{which =} \StringTok{"all"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## please wait...calculating quantiles...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f\textless{}{-}}\KeywordTok{ugarchforecast}\NormalTok{(}\DataTypeTok{fitORspec =}\NormalTok{ m,}\DataTypeTok{n.ahead =} \DecValTok{60}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{fitted}\NormalTok{(f))}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{sigma}\NormalTok{(f))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-13-1} \includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-13-2} \includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-13-3} \end{center}

GJR-GARCH in mean

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{s\textless{}{-}}\KeywordTok{ugarchspec}\NormalTok{(}\DataTypeTok{mean.model =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{armaOrder=}\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{),}\DataTypeTok{archm=}\NormalTok{T,}\DataTypeTok{archpow=}\DecValTok{2}\NormalTok{),}\DataTypeTok{variance.model =} \KeywordTok{list}\NormalTok{(}\DataTypeTok{model=}\StringTok{"sGARCH"}\NormalTok{)}
\NormalTok{              ,}\DataTypeTok{distribution.model =} \StringTok{"sstd"}\NormalTok{)}
\NormalTok{m\textless{}{-}}\KeywordTok{ugarchfit}\NormalTok{(}\DataTypeTok{data=}\NormalTok{return,}\DataTypeTok{spec=}\NormalTok{s)}
\NormalTok{m}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## *---------------------------------*
## *          GARCH Model Fit        *
## *---------------------------------*
## 
## Conditional Variance Dynamics    
## -----------------------------------
## GARCH Model  : sGARCH(1,1)
## Mean Model   : ARFIMA(0,0,0)
## Distribution : sstd 
## 
## Optimal Parameters
## ------------------------------------
##         Estimate  Std. Error  t value Pr(>|t|)
## mu     -0.000548    0.000561 -0.97667 0.328731
## archm   2.516780    1.267853  1.98507 0.047136
## omega   0.000005    0.000003  1.34986 0.177062
## alpha1  0.063407    0.013410  4.72831 0.000002
## beta1   0.930139    0.015162 61.34597 0.000000
## skew    1.026251    0.031792 32.28044 0.000000
## shape   5.145742    0.571638  9.00175 0.000000
## 
## Robust Standard Errors:
##         Estimate  Std. Error  t value Pr(>|t|)
## mu     -0.000548    0.000547 -1.00133 0.316667
## archm   2.516780    1.167343  2.15599 0.031084
## omega   0.000005    0.000009  0.53121 0.595276
## alpha1  0.063407    0.030464  2.08136 0.037401
## beta1   0.930139    0.036857 25.23623 0.000000
## skew    1.026251    0.029082 35.28832 0.000000
## shape   5.145742    0.648904  7.92990 0.000000
## 
## LogLikelihood : 5155.77 
## 
## Information Criteria
## ------------------------------------
##                     
## Akaike       -5.0727
## Bayes        -5.0533
## Shibata      -5.0727
## Hannan-Quinn -5.0656
## 
## Weighted Ljung-Box Test on Standardized Residuals
## ------------------------------------
##                         statistic  p-value
## Lag[1]                      10.74 0.001049
## Lag[2*(p+q)+(p+q)-1][2]     11.02 0.001027
## Lag[4*(p+q)+(p+q)-1][5]     11.48 0.003805
## d.o.f=0
## H0 : No serial correlation
## 
## Weighted Ljung-Box Test on Standardized Squared Residuals
## ------------------------------------
##                         statistic p-value
## Lag[1]                      2.069  0.1503
## Lag[2*(p+q)+(p+q)-1][5]     3.234  0.3660
## Lag[4*(p+q)+(p+q)-1][9]     5.495  0.3604
## d.o.f=2
## 
## Weighted ARCH LM Tests
## ------------------------------------
##             Statistic Shape Scale P-Value
## ARCH Lag[3]   0.05879 0.500 2.000  0.8084
## ARCH Lag[5]   3.18122 1.440 1.667  0.2646
## ARCH Lag[7]   4.31781 2.315 1.543  0.3031
## 
## Nyblom stability test
## ------------------------------------
## Joint Statistic:  4.0828
## Individual Statistics:              
## mu     0.11797
## archm  0.35490
## omega  0.41429
## alpha1 0.24075
## beta1  0.44941
## skew   0.06264
## shape  0.56250
## 
## Asymptotic Critical Values (10% 5% 1%)
## Joint Statistic:          1.69 1.9 2.35
## Individual Statistic:     0.35 0.47 0.75
## 
## Sign Bias Test
## ------------------------------------
##                    t-value   prob sig
## Sign Bias           0.4536 0.6502    
## Negative Sign Bias  1.5163 0.1296    
## Positive Sign Bias  1.0083 0.3134    
## Joint Effect        3.4130 0.3322    
## 
## 
## Adjusted Pearson Goodness-of-Fit Test:
## ------------------------------------
##   group statistic p-value(g-1)
## 1    20     25.37      0.14874
## 2    30     32.70      0.29002
## 3    40     45.82      0.21009
## 4    50     63.60      0.07853
## 
## 
## Elapsed time : 1.19911
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{plot}\NormalTok{(m,}\DataTypeTok{which =} \StringTok{"all"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## please wait...calculating quantiles...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{f\textless{}{-}}\KeywordTok{ugarchforecast}\NormalTok{(}\DataTypeTok{fitORspec =}\NormalTok{ m,}\DataTypeTok{n.ahead =} \DecValTok{60}\NormalTok{)}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{fitted}\NormalTok{(f))}
\KeywordTok{plot}\NormalTok{(}\KeywordTok{sigma}\NormalTok{(f))}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-14-1} \includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-14-2} \includegraphics{bookdown-demo_files/figure-latex/unnamed-chunk-14-3} \end{center}

\hypertarget{conclusuxe3o}{%
\chapter*{Conclus√£o}\label{conclusuxe3o}}
\addcontentsline{toc}{chapter}{Conclus√£o}

We have finished a nice book.

  \bibliography{book.bib,packages.bib}

\part*{Ap√™ndices}
\addcontentsline{toc}{part}{Ap√™ndices}

\newpage
\part*{\normalfont\huge\bfseries\centering Ap√™ndice I}
\newpage

\part*{\normalfont\huge\bfseries\centering Ap√™ndice II}
\newpage

\end{document}
